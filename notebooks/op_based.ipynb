{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.optim\n",
    "from abc import abstractmethod\n",
    "from collections import defaultdict\n",
    "from functools import lru_cache\n",
    "from itertools import count\n",
    "from typing import List, Dict\n",
    "from typing import Tuple, Any\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.nn import MSELoss, ReLU\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "from config import Config\n",
    "import config\n",
    "from data import MDataset, Graph, load_graphs, save_dataset_pkl, load_dataset_pkl, save_scalers_pkl, load_scalers_pkl\n",
    "import data\n",
    "from base_module import MModule\n",
    "import base_module\n",
    "from executor import single_train_loop, nested_detach\n",
    "import executor\n",
    "from objects import ModelType\n",
    "import objects\n",
    "from metric import MetricUtil\n",
    "import metric\n",
    "from logger import init_logging\n",
    "import logger\n",
    "reload(config)\n",
    "reload(data)\n",
    "reload(base_module)\n",
    "reload(executor)\n",
    "reload(objects)\n",
    "reload(metric)\n",
    "reload(logger)\n",
    "from config import Config\n",
    "from data import MDataset, Graph, load_graphs\n",
    "from base_module import MModule\n",
    "from objects import ModelType\n",
    "from metric import MetricUtil\n",
    "from logger import init_logging\n",
    "init_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_environment_str = \"RTX2080Ti_CPU100\"\n",
    "normalizer_cls = StandardScaler # MinMaxScaler\n",
    "dummy = False\n",
    "model_type = ModelType.MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eval_graphs = load_graphs(dataset_environment_str,\n",
    "                            train_or_eval=\"train\",\n",
    "                            use_dummy=dummy,\n",
    "                            max_row=1_000)\n",
    "train_graphs = load_graphs(dataset_environment_str,\n",
    "                            train_or_eval=\"train\",\n",
    "                            use_dummy=dummy,\n",
    "                            max_row=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_configs = {\n",
    "    ModelType.MLP.name: Config.from_dict({\n",
    "        \"model\": \"MLP\",\n",
    "        \"all_seed\": 42,\n",
    "        \"dataset_environment_str\": \"RTX2080Ti_CPU100\",\n",
    "        \"dataset_normalization\": \"Standard\",\n",
    "        \"dataset_params\": {\n",
    "            \"duration_summed\": False,\n",
    "        },\n",
    "        \"dataset_dummy\": False,\n",
    "        \"batch_size\": 512,\n",
    "        \"eval_steps\": 500,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"epochs\": 5,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"meta_configs\": {\n",
    "            \"learning_rate\": 0.005,\n",
    "            \"meta_learning_rate\": 0.001,\n",
    "            \"meta_train_steps\": 1000,\n",
    "            \"meta_task_per_step\": 8,\n",
    "            \"meta_fast_adaption_step\": 5,\n",
    "            \"meta_dataset_train_environment_strs\": [\"RTX2080Ti_CPU100\"],\n",
    "            \"meta_dataset_eval_environment_strs\": [\"RTX2080Ti_CPU100\"],\n",
    "        },\n",
    "    }),\n",
    "    ModelType.PerfNet.name: Config.from_dict({\n",
    "        \"model\": \"PerfNet\",\n",
    "        \"dataset_environment_str\": \"RTX2080Ti_CPU100\",\n",
    "        \"dataset_normalization\": \"Standard\",\n",
    "        \"all_seed\": 42,\n",
    "        \"dataset_params\": {\n",
    "            \"duration_summed\": False,\n",
    "        },\n",
    "        \"dataset_dummy\": True,\n",
    "        \"batch_size\": 16,\n",
    "        \"eval_steps\": 100,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"epochs\": 100,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"meta_configs\": {\n",
    "            \"learning_rate\": 0.005,\n",
    "            \"meta_learning_rate\": 0.001,\n",
    "            \"meta_train_steps\": 1000,\n",
    "            \"meta_task_per_step\": 8,\n",
    "            \"meta_fast_adaption_step\": 5,\n",
    "            \"meta_dataset_train_environment_strs\": [\"RTX2080Ti_CPU100\"],\n",
    "            \"meta_dataset_eval_environment_strs\": [\"RTX2080Ti_CPU100\"],\n",
    "        },\n",
    "    })\n",
    "}\n",
    "\n",
    "conf = train_configs[model_type.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_dataset(graphs: List[Graph]) -> MDataset:\n",
    "    op_X, op_Y = list(), list()\n",
    "    data_idx_to_graph = dict()\n",
    "    counter = iter(count())\n",
    "    op_feature_len = 0\n",
    "\n",
    "    def node_features(g: Graph) -> Tuple[\n",
    "        List[Dict], List[Dict]]:\n",
    "        X, Y = list(), list()\n",
    "        for i, node in enumerate(g.nodes):\n",
    "            x_op_feature = node.op.to_feature_array(\"complex\")\n",
    "            x = {\n",
    "                \"x_op_feature\": x_op_feature\n",
    "            }\n",
    "            node_durations = (node.duration,)\n",
    "\n",
    "            x[\"x_id\"] = i\n",
    "            x[\"x_graph_id\"] = g.ID\n",
    "            y = {\"y_node_durations\": node_durations, \"y_id\": i, \"y_graph_id\": g.ID}\n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "        return X, Y\n",
    "\n",
    "    for graph in graphs:\n",
    "        X, Y = node_features(graph)\n",
    "        for x in X:\n",
    "            # if len(x['x_op_feature'])!=37:\n",
    "            #     print(x['x_graph_id'], len(x['x_op_feature']))\n",
    "            op_feature_len = max(op_feature_len, len(x[\"x_op_feature\"]))\n",
    "        op_X.extend(X)\n",
    "        op_Y.extend(Y)\n",
    "        for i in range(len(X)):\n",
    "            data_idx_to_graph[next(counter)] = graph\n",
    "    for x in op_X:\n",
    "        v = x[\"x_op_feature\"]\n",
    "        x[\"x_op_feature\"] = np.pad(v, (0, op_feature_len - v.size))\n",
    "\n",
    "    dataset = MDataset(op_X, op_Y)\n",
    "    return dataset\n",
    "\n",
    "train_ds = init_dataset(train_graphs)\n",
    "eval_ds = init_dataset(eval_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scalers(ds):\n",
    "    scaler_cls = normalizer_cls\n",
    "    op_feature_array = list()\n",
    "    y_array = list()\n",
    "\n",
    "    for data in ds:\n",
    "        feature, label = data\n",
    "        op_feature_array.append(feature[\"x_op_feature\"])\n",
    "        y_array.append(label[\"y_node_durations\"])\n",
    "\n",
    "    op_feature_array = np.array(op_feature_array)\n",
    "    y_array = np.array(y_array)\n",
    "\n",
    "    op_feature_scaler = scaler_cls()\n",
    "    op_feature_scaler.fit(op_feature_array)\n",
    "\n",
    "    y_scaler = scaler_cls()\n",
    "    y_scaler.fit(y_array)\n",
    "    return op_feature_scaler, y_scaler\n",
    "\n",
    "scalers = get_scalers(train_ds)\n",
    "op_feature_scaler, y_scaler = scalers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_dataset(ds: MDataset) -> MDataset:\n",
    "    op_feature_array = list()\n",
    "    y_array = list()\n",
    "\n",
    "    for data in ds:\n",
    "        feature, label = data\n",
    "        op_feature_array.append(feature[\"x_op_feature\"])\n",
    "        y_array.append(label[\"y_node_durations\"])\n",
    "\n",
    "    op_feature_array = np.array(op_feature_array, dtype=np.float32)\n",
    "    y_array = np.array(y_array, dtype=np.float32)\n",
    "\n",
    "\n",
    "    op_feature_array = op_feature_scaler.transform(op_feature_array)\n",
    "    y_array = y_scaler.transform(y_array)\n",
    "\n",
    "    processed_features = list()\n",
    "    processed_labels = list()\n",
    "    for i, data in enumerate(ds):\n",
    "        feature, label = data\n",
    "        processed_features.append({\n",
    "            \"x_id\": feature[\"x_id\"],\n",
    "            \"x_graph_id\": feature[\"x_graph_id\"],\n",
    "            # 运行时再传到cuda那边\n",
    "            # \"x_op_feature\": torch.Tensor(op_feature_array[i]).to(device=self.conf.device)\n",
    "            \"x_op_feature\": torch.Tensor(op_feature_array[i])\n",
    "        })\n",
    "        processed_labels.append({\n",
    "            \"y_id\": label[\"y_id\"],\n",
    "            \"y_graph_id\": label[\"y_graph_id\"],\n",
    "            # \"y_node_durations\": torch.Tensor(y_array[i]).to(device=self.conf.device)\n",
    "            \"y_node_durations\": torch.Tensor(y_array[i])\n",
    "        })\n",
    "\n",
    "    ds = MDataset(processed_features, processed_labels)\n",
    "    return ds\n",
    "\n",
    "preprocessed_train_ds = preprocess_dataset(train_ds)\n",
    "preprocessed_eval_ds = preprocess_dataset(eval_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset_pkl(preprocessed_train_ds, conf.dataset_environment, \"OpBased\", 'train',\n",
    "                         conf.dataset_normalization)\n",
    "save_dataset_pkl(preprocessed_eval_ds, conf.dataset_environment, \"OpBased\", 'eval',\n",
    "                         conf.dataset_normalization)\n",
    "save_scalers_pkl(scalers, conf.dataset_environment, \"OpBased\", 'train',\n",
    "                         conf.dataset_normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_train_ds = load_dataset_pkl(conf.dataset_environment, \"OpBased\", 'train', \n",
    "                                         conf.dataset_normalization)\n",
    "preprocessed_eval_ds = load_dataset_pkl(conf.dataset_environment, \"OpBased\", 'eval',\n",
    "                                        conf.dataset_normalization)\n",
    "scalers = load_scalers_pkl(conf.dataset_environment, \"OpBased\", 'train',\n",
    "                           conf.dataset_normalization)\n",
    "op_feature_scaler, y_scaler = scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_evaluate_metrics(input_batches, output_batches, eval_loss) -> Dict[str, float]:\n",
    "    batches_len = len(input_batches)\n",
    "\n",
    "    def compute_op_durations(_logits):\n",
    "        transformed: np.ndarray = y_scaler.inverse_transform(_logits)\n",
    "        duration_dim = (0, 3)\n",
    "        durations = transformed[:, duration_dim[0]:duration_dim[1]].sum(axis=1)\n",
    "        return durations\n",
    "\n",
    "    graph_id_to_duration_pred = defaultdict(int)\n",
    "    for idx in range(batches_len):\n",
    "        inputs = input_batches[idx]\n",
    "        logits = output_batches[idx]\n",
    "        logits = nested_detach(logits)\n",
    "        logits = logits.cpu().numpy()\n",
    "        graph_ids = inputs[\"x_graph_id\"]\n",
    "        op_durations = compute_op_durations(logits)\n",
    "        for i, graph_id in enumerate(graph_ids):\n",
    "            op_duration = op_durations[i].item()\n",
    "            graph_id_to_duration_pred[graph_id] += op_duration\n",
    "    duration_metrics = MetricUtil.compute_duration_metrics(eval_graphs, graph_id_to_duration_pred)\n",
    "    return {\"eval_loss\": eval_loss, **duration_metrics}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def to_device(conf, features, labels):\n",
    "    features['x_op_feature'] = features[\"x_op_feature\"].to(device=conf.device)\n",
    "    labels['y_node_durations'] = labels['y_node_durations'].to(device=conf.device)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModel(MModule):\n",
    "\n",
    "    @staticmethod\n",
    "    def dimension_len(t):\n",
    "        return t[-1] - t[0]\n",
    "\n",
    "    def __init__(self, input_dimension, output_dimension, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.input = torch.nn.Linear(input_dimension, 128)\n",
    "        self.relu1 = ReLU()\n",
    "        self.dense1 = torch.nn.Linear(128, 64)\n",
    "        self.relu2 = ReLU()\n",
    "        self.dense2 = torch.nn.Linear(64, 32)\n",
    "        self.relu3 = ReLU()\n",
    "        self.output = torch.nn.Linear(32, output_dimension)\n",
    "        self.loss_fn = MSELoss()\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X[\"x_op_feature\"]\n",
    "        X = self.input(X)\n",
    "        X = self.relu1(X)\n",
    "        X = self.dense1(X)\n",
    "        X = self.relu2(X)\n",
    "        X = self.dense2(X)\n",
    "        X = self.relu3(X)\n",
    "        Y = self.output(X)\n",
    "        return Y\n",
    "\n",
    "    def compute_loss(self, outputs, Y):\n",
    "        node_durations = Y[\"y_node_durations\"]\n",
    "        loss = self.loss_fn(outputs, node_durations)\n",
    "        return loss\n",
    "\n",
    "\n",
    "def init_MLP_model(self) -> MModule | Any:\n",
    "    sample_preprocessed_ds = preprocessed_train_ds\n",
    "    sample_x_dict = sample_preprocessed_ds.features[0]\n",
    "    sample_y_dict = sample_preprocessed_ds.labels[0]\n",
    "    return MLPModel(input_dimension=len(sample_x_dict[\"x_op_feature\"]),\n",
    "                    output_dimension=len(sample_y_dict[\"y_node_durations\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PerfNetModel(MModule):\n",
    "    @staticmethod\n",
    "    def dimension_len(t):\n",
    "        return t[-1] - t[0]\n",
    "\n",
    "    def __init__(self, output_dimension, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.conv1 = torch.nn.LazyConv1d(out_channels=32, kernel_size=3, bias=True, padding_mode='zeros')\n",
    "        self.conv2 = torch.nn.LazyConv1d(out_channels=128, kernel_size=2, bias=True, padding_mode='zeros')\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.dense1 = torch.nn.LazyLinear(32)\n",
    "        self.relu1 = ReLU()\n",
    "        self.dense2 = torch.nn.LazyLinear(64)\n",
    "        self.relu2 = ReLU()\n",
    "        self.dense3 = torch.nn.LazyLinear(128)\n",
    "        self.relu3 = ReLU()\n",
    "        self.dense4 = torch.nn.LazyLinear(256)\n",
    "        self.relu4 = ReLU()\n",
    "        self.dropout = torch.nn.Dropout(p=0.3)\n",
    "        self.output = torch.nn.LazyLinear(output_dimension)\n",
    "        self.loss_fn = MSELoss()\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X[\"x_op_feature\"]\n",
    "        X = torch.unsqueeze(X, dim=1)\n",
    "        X = self.conv1(X)\n",
    "        X = self.conv2(X)\n",
    "        X = self.flatten(X)\n",
    "        X = self.dense1(X)\n",
    "        X = self.relu1(X)\n",
    "        X = self.dense2(X)\n",
    "        X = self.relu2(X)\n",
    "        X = self.dense3(X)\n",
    "        X = self.relu3(X)\n",
    "        X = self.dense4(X)\n",
    "        X = self.relu4(X)\n",
    "        X = self.dropout(X)\n",
    "        Y = self.output(X)\n",
    "        return Y\n",
    "\n",
    "    def compute_loss(self, outputs, Y):\n",
    "        node_durations = Y[\"y_node_durations\"]\n",
    "        loss = self.loss_fn(outputs, node_durations)\n",
    "        return loss\n",
    "\n",
    "\n",
    "def init_PerfNet_model(self) -> MModule | Any:\n",
    "    sample_y_dict = preprocessed_train_ds.labels[0]\n",
    "    return PerfNetModel(output_dimension=len(sample_y_dict[\"y_node_durations\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "init_model_funcs = {\n",
    "    ModelType.MLP.name: init_MLP_model,\n",
    "    ModelType.PerfNet.name: init_PerfNet_model,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_model = init_model_funcs[model_type.name]\n",
    "\n",
    "model = init_model(conf)\n",
    "model = model.to(conf.device)\n",
    "\n",
    "single_train_loop(model_type, conf, preprocessed_train_ds, preprocessed_eval_ds, model, compute_evaluate_metrics, to_device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLT-perf-model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

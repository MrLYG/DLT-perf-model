{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.optim\n",
    "from abc import abstractmethod\n",
    "from collections import defaultdict\n",
    "from functools import lru_cache\n",
    "from itertools import count\n",
    "from typing import List, Dict\n",
    "from typing import Tuple, Any\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.nn import MSELoss, ReLU\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets_path: /home/yangzichao/repos/DLT-perf-model/datasets\n",
      "configs_path: /home/yangzichao/repos/DLT-perf-model/notebooks/configs\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "from config import Config\n",
    "import config\n",
    "from data import MDataset, Graph, load_graphs\n",
    "import data\n",
    "from base_module import MModule\n",
    "import base_module\n",
    "from executor import single_train_loop, nested_detach\n",
    "import executor\n",
    "from objects import ModelType\n",
    "import objects\n",
    "from metric import MetricUtil\n",
    "import metric\n",
    "from logger import init_logging\n",
    "import logger\n",
    "reload(config)\n",
    "reload(data)\n",
    "reload(base_module)\n",
    "reload(executor)\n",
    "reload(objects)\n",
    "reload(metric)\n",
    "reload(logger)\n",
    "from config import Config\n",
    "from data import MDataset, Graph, load_graphs\n",
    "from base_module import MModule\n",
    "from objects import ModelType\n",
    "from metric import MetricUtil\n",
    "from logger import init_logging\n",
    "init_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-05 22:49:16,616] {data.py:330} INFO - Loading graphs train\n",
      "[2023-12-05 22:49:16,617] {data.py:301} INFO - Loading merged.csv\n",
      "[2023-12-05 22:49:16,637] {data.py:304} INFO - Loaded merged.csv, 10000 rows\n",
      "[2023-12-05 22:49:17,533] {data.py:310} INFO - Loaded rand_14500.143_7.csv, 2603 rows\n",
      "[2023-12-05 22:49:17,807] {data.py:310} INFO - Loaded mnasnet0_75.115_7.csv, 769 rows\n",
      "[2023-12-05 22:49:18,023] {data.py:310} INFO - Loaded vit_b_16.102_7.csv, 611 rows\n",
      "[2023-12-05 22:49:20,070] {data.py:310} INFO - Loaded rand_3000.231_7.csv, 6017 rows\n",
      "[2023-12-05 22:49:20,071] {data.py:330} INFO - Loading graphs train\n",
      "[2023-12-05 22:49:20,072] {data.py:301} INFO - Loading merged.csv\n",
      "[2023-12-05 22:49:20,229] {data.py:304} INFO - Loaded merged.csv, 100000 rows\n",
      "[2023-12-05 22:49:21,187] {data.py:310} INFO - Loaded rand_14500.143_7.csv, 2603 rows\n",
      "[2023-12-05 22:49:21,449] {data.py:310} INFO - Loaded mnasnet0_75.115_7.csv, 769 rows\n",
      "[2023-12-05 22:49:21,658] {data.py:310} INFO - Loaded vit_b_16.102_7.csv, 611 rows\n",
      "[2023-12-05 22:49:24,530] {data.py:310} INFO - Loaded rand_3000.231_7.csv, 8475 rows\n",
      "[2023-12-05 22:49:25,384] {data.py:310} INFO - Loaded rand_14500.106_7.csv, 2548 rows\n",
      "[2023-12-05 22:49:25,437] {data.py:310} INFO - Loaded vgg13.38_7.csv, 147 rows\n",
      "[2023-12-05 22:49:25,699] {data.py:310} INFO - Loaded mnasnet0_75.150_7.csv, 769 rows\n",
      "[2023-12-05 22:49:25,773] {data.py:310} INFO - Loaded vgg19.66_7.csv, 209 rows\n",
      "[2023-12-05 22:49:26,061] {data.py:310} INFO - Loaded shufflenet_v2_x1_5.91_7.csv, 847 rows\n",
      "[2023-12-05 22:49:26,527] {data.py:310} INFO - Loaded rand_1000.146_7.csv, 1366 rows\n",
      "[2023-12-05 22:49:27,194] {data.py:310} INFO - Loaded rand_15500.65_7.csv, 1979 rows\n",
      "[2023-12-05 22:49:27,532] {data.py:310} INFO - Loaded rand_15500.124_7.csv, 991 rows\n",
      "[2023-12-05 22:49:28,041] {data.py:310} INFO - Loaded rand_1000.103_7.csv, 1236 rows\n",
      "[2023-12-05 22:49:28,896] {data.py:310} INFO - Loaded rand_16500.234_7.csv, 2529 rows\n",
      "[2023-12-05 22:49:29,764] {data.py:310} INFO - Loaded rand_2000.256_7.csv, 2573 rows\n",
      "[2023-12-05 22:49:30,481] {data.py:310} INFO - Loaded rand_15500.20_7.csv, 1959 rows\n",
      "[2023-12-05 22:49:31,203] {data.py:310} INFO - Loaded rand_1000.44_7.csv, 1960 rows\n",
      "[2023-12-05 22:49:31,458] {data.py:310} INFO - Loaded rand_15500.161_7.csv, 680 rows\n",
      "[2023-12-05 22:49:31,696] {data.py:310} INFO - Loaded rand_9500.94_7.csv, 682 rows\n",
      "[2023-12-05 22:49:32,392] {data.py:310} INFO - Loaded swin_b.19_7.csv, 2014 rows\n",
      "[2023-12-05 22:49:32,666] {data.py:310} INFO - Loaded mnasnet0_75.42_7.csv, 771 rows\n",
      "[2023-12-05 22:49:35,655] {data.py:310} INFO - Loaded rand_3500.110_7.csv, 8606 rows\n",
      "[2023-12-05 22:49:36,766] {data.py:310} INFO - Loaded rand_8000.149_7.csv, 2877 rows\n",
      "[2023-12-05 22:49:36,866] {data.py:310} INFO - Loaded vgg16_bn.24_7.csv, 279 rows\n",
      "[2023-12-05 22:49:37,641] {data.py:310} INFO - Loaded efficientnet_b4.125_7.csv, 2232 rows\n",
      "[2023-12-05 22:49:38,420] {data.py:310} INFO - Loaded efficientnet_b4.90_7.csv, 2232 rows\n",
      "[2023-12-05 22:49:38,841] {data.py:310} INFO - Loaded vit_l_16.43_7.csv, 1187 rows\n",
      "[2023-12-05 22:49:39,219] {data.py:310} INFO - Loaded rand_8500.42_7.csv, 1066 rows\n",
      "[2023-12-05 22:49:39,594] {data.py:310} INFO - Loaded rand_3500.155_7.csv, 1075 rows\n",
      "[2023-12-05 22:49:39,665] {data.py:310} INFO - Loaded vgg11_bn.49_7.csv, 193 rows\n",
      "[2023-12-05 22:49:39,766] {data.py:310} INFO - Loaded vgg16_bn.61_7.csv, 283 rows\n",
      "[2023-12-05 22:49:40,459] {data.py:310} INFO - Loaded rand_8500.194_7.csv, 1985 rows\n",
      "[2023-12-05 22:49:41,016] {data.py:310} INFO - Loaded resnet101.46_7.csv, 1600 rows\n",
      "[2023-12-05 22:49:41,555] {data.py:310} INFO - Loaded wide_resnet101_2.96_7.csv, 1598 rows\n",
      "[2023-12-05 22:49:42,516] {data.py:310} INFO - Loaded rand_2500.177_7.csv, 2873 rows\n",
      "[2023-12-05 22:49:42,994] {data.py:310} INFO - Loaded swin_v2_t.55_7.csv, 1415 rows\n",
      "[2023-12-05 22:49:43,909] {data.py:310} INFO - Loaded efficientnet_b5.90_7.csv, 2707 rows\n",
      "[2023-12-05 22:49:44,142] {data.py:310} INFO - Loaded rand_1500.222_7.csv, 681 rows\n",
      "[2023-12-05 22:49:44,803] {data.py:310} INFO - Loaded rand_16500.46_7.csv, 1956 rows\n",
      "[2023-12-05 22:49:45,088] {data.py:310} INFO - Loaded convnext_tiny.64_7.csv, 837 rows\n",
      "[2023-12-05 22:49:45,734] {data.py:310} INFO - Loaded swin_b.103_7.csv, 1915 rows\n",
      "[2023-12-05 22:49:45,982] {data.py:310} INFO - Loaded rand_2000.22_7.csv, 724 rows\n",
      "[2023-12-05 22:49:46,389] {data.py:310} INFO - Loaded rand_16000.115_7.csv, 1205 rows\n",
      "[2023-12-05 22:49:46,677] {data.py:310} INFO - Loaded shufflenet_v2_x1_5.119_7.csv, 847 rows\n",
      "[2023-12-05 22:49:47,159] {data.py:310} INFO - Loaded swin_v2_t.10_7.csv, 1428 rows\n",
      "[2023-12-05 22:49:48,086] {data.py:310} INFO - Loaded rand_2500.132_7.csv, 2315 rows\n",
      "[2023-12-05 22:49:48,495] {data.py:310} INFO - Loaded rand_3000.188_7.csv, 1211 rows\n",
      "[2023-12-05 22:49:48,779] {data.py:310} INFO - Loaded convnext_tiny.21_7.csv, 839 rows\n",
      "[2023-12-05 22:49:49,656] {data.py:310} INFO - Loaded rand_2000.67_7.csv, 2608 rows\n",
      "[2023-12-05 22:49:50,328] {data.py:310} INFO - Loaded swin_s.50_7.csv, 1955 rows\n",
      "[2023-12-05 22:49:51,625] {data.py:310} INFO - Loaded rand_500.56_7.csv, 3807 rows\n",
      "[2023-12-05 22:49:52,898] {data.py:310} INFO - Loaded rand_14500.242_7.csv, 3808 rows\n",
      "[2023-12-05 22:49:53,230] {data.py:310} INFO - Loaded rand_3000.175_7.csv, 974 rows\n",
      "[2023-12-05 22:49:54,146] {data.py:310} INFO - Loaded efficientnet_b5.28_7.csv, 2707 rows\n",
      "[2023-12-05 22:49:54,833] {data.py:310} INFO - Loaded swin_s.15_7.csv, 2038 rows\n",
      "[2023-12-05 22:49:54,911] {data.py:310} INFO - Loaded rand_11500.93_7.csv, 219 rows\n"
     ]
    }
   ],
   "source": [
    "dataset_environment_str = \"RTX2080Ti_CPU100\"\n",
    "normalizer_cls = StandardScaler # MinMaxScaler\n",
    "dummy = False\n",
    "\n",
    "eval_graphs = load_graphs(dataset_environment_str,\n",
    "                            train_or_eval=\"train\",\n",
    "                            use_dummy=dummy,\n",
    "                            max_row=100_000)\n",
    "train_graphs = load_graphs(dataset_environment_str,\n",
    "                            train_or_eval=\"train\",\n",
    "                            use_dummy=dummy,\n",
    "                            max_row=1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_configs = {\n",
    "    ModelType.MLP: Config.from_dict({\n",
    "        \"model\": \"MLP\",\n",
    "        \"all_seed\": 42,\n",
    "        \"dataset_environment_str\": \"RTX2080Ti_CPU100\",\n",
    "        \"dataset_normalization\": \"Standard\",\n",
    "        \"dataset_params\": {\n",
    "            \"duration_summed\": False,\n",
    "        },\n",
    "        \"dataset_dummy\": False,\n",
    "        \"batch_size\": 512,\n",
    "        \"eval_steps\": 500,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"epochs\": 5,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"meta_configs\": {\n",
    "            \"learning_rate\": 0.005,\n",
    "            \"meta_learning_rate\": 0.001,\n",
    "            \"meta_train_steps\": 1000,\n",
    "            \"meta_task_per_step\": 8,\n",
    "            \"meta_fast_adaption_step\": 5,\n",
    "            \"meta_dataset_train_environment_strs\": [\"RTX2080Ti_CPU100\"],\n",
    "            \"meta_dataset_eval_environment_strs\": [\"RTX2080Ti_CPU100\"],\n",
    "        },\n",
    "    }),\n",
    "    ModelType.PerfNet: Config.from_dict({\n",
    "        \"model\": \"PerfNet\",\n",
    "        \"dataset_environment_str\": \"RTX2080Ti_CPU100\",\n",
    "        \"dataset_normalization\": \"Standard\",\n",
    "        \"all_seed\": 42,\n",
    "        \"dataset_params\": {\n",
    "            \"duration_summed\": False,\n",
    "        },\n",
    "        \"dataset_dummy\": True,\n",
    "        \"batch_size\": 16,\n",
    "        \"eval_steps\": 100,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"epochs\": 100,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"meta_configs\": {\n",
    "            \"learning_rate\": 0.005,\n",
    "            \"meta_learning_rate\": 0.001,\n",
    "            \"meta_train_steps\": 1000,\n",
    "            \"meta_task_per_step\": 8,\n",
    "            \"meta_fast_adaption_step\": 5,\n",
    "            \"meta_dataset_train_environment_strs\": [\"RTX2080Ti_CPU100\"],\n",
    "            \"meta_dataset_eval_environment_strs\": [\"RTX2080Ti_CPU100\"],\n",
    "        },\n",
    "    })\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_dataset(graphs: List[Graph]) -> MDataset:\n",
    "    op_X, op_Y = list(), list()\n",
    "    data_idx_to_graph = dict()\n",
    "    counter = iter(count())\n",
    "    op_feature_len = 0\n",
    "\n",
    "    def node_features(g: Graph) -> Tuple[\n",
    "        List[Dict], List[Dict]]:\n",
    "        X, Y = list(), list()\n",
    "        for i, node in enumerate(g.nodes):\n",
    "            x_op_feature = node.op.to_feature_array(\"complex\")\n",
    "            x = {\n",
    "                \"x_op_feature\": x_op_feature\n",
    "            }\n",
    "            node_durations = (node.duration,)\n",
    "\n",
    "            x[\"x_id\"] = i\n",
    "            x[\"x_graph_id\"] = g.ID\n",
    "            y = {\"y_node_durations\": node_durations, \"y_id\": i, \"y_graph_id\": g.ID}\n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "        return X, Y\n",
    "\n",
    "    for graph in graphs:\n",
    "        X, Y = node_features(graph)\n",
    "        for x in X:\n",
    "            # if len(x['x_op_feature'])!=37:\n",
    "            #     print(x['x_graph_id'], len(x['x_op_feature']))\n",
    "            op_feature_len = max(op_feature_len, len(x[\"x_op_feature\"]))\n",
    "        op_X.extend(X)\n",
    "        op_Y.extend(Y)\n",
    "        for i in range(len(X)):\n",
    "            data_idx_to_graph[next(counter)] = graph\n",
    "    for x in op_X:\n",
    "        v = x[\"x_op_feature\"]\n",
    "        x[\"x_op_feature\"] = np.pad(v, (0, op_feature_len - v.size))\n",
    "\n",
    "    dataset = MDataset(op_X, op_Y)\n",
    "    return dataset\n",
    "\n",
    "train_ds = init_dataset(train_graphs)\n",
    "eval_ds = init_dataset(eval_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scalers(ds):\n",
    "    scaler_cls = normalizer_cls\n",
    "    op_feature_array = list()\n",
    "    y_array = list()\n",
    "\n",
    "    for data in ds:\n",
    "        feature, label = data\n",
    "        op_feature_array.append(feature[\"x_op_feature\"])\n",
    "        y_array.append(label[\"y_node_durations\"])\n",
    "\n",
    "    op_feature_array = np.array(op_feature_array)\n",
    "    y_array = np.array(y_array)\n",
    "\n",
    "    op_feature_scaler = scaler_cls()\n",
    "    op_feature_scaler.fit(op_feature_array)\n",
    "\n",
    "    y_scaler = scaler_cls()\n",
    "    y_scaler.fit(y_array)\n",
    "    return op_feature_scaler, y_scaler\n",
    "\n",
    "op_feature_scaler, y_scaler = get_scalers(train_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_dataset(ds: MDataset) -> MDataset:\n",
    "    op_feature_array = list()\n",
    "    y_array = list()\n",
    "\n",
    "    for data in ds:\n",
    "        feature, label = data\n",
    "        op_feature_array.append(feature[\"x_op_feature\"])\n",
    "        y_array.append(label[\"y_node_durations\"])\n",
    "\n",
    "    op_feature_array = np.array(op_feature_array, dtype=np.float16)\n",
    "    y_array = np.array(y_array, dtype=np.float16)\n",
    "\n",
    "\n",
    "    op_feature_array = op_feature_scaler.transform(op_feature_array)\n",
    "    y_array = y_scaler.transform(y_array)\n",
    "\n",
    "    processed_features = list()\n",
    "    processed_labels = list()\n",
    "    for i, data in enumerate(ds):\n",
    "        feature, label = data\n",
    "        processed_features.append({\n",
    "            \"x_id\": feature[\"x_id\"],\n",
    "            \"x_graph_id\": feature[\"x_graph_id\"],\n",
    "            # 运行时再传到cuda那边\n",
    "            # \"x_op_feature\": torch.Tensor(op_feature_array[i]).to(device=self.conf.device)\n",
    "            \"x_op_feature\": torch.Tensor(op_feature_array[i])\n",
    "        })\n",
    "        processed_labels.append({\n",
    "            \"y_id\": label[\"y_id\"],\n",
    "            \"y_graph_id\": label[\"y_graph_id\"],\n",
    "            # \"y_node_durations\": torch.Tensor(y_array[i]).to(device=self.conf.device)\n",
    "            \"y_node_durations\": torch.Tensor(y_array[i])\n",
    "        })\n",
    "\n",
    "    ds = MDataset(processed_features, processed_labels)\n",
    "    return ds\n",
    "\n",
    "preprocessed_train_ds = preprocess_dataset(train_ds)\n",
    "preprocessed_eval_ds = preprocess_dataset(eval_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_evaluate_metrics(input_batches, output_batches, eval_loss) -> Dict[str, float]:\n",
    "    batches_len = len(input_batches)\n",
    "\n",
    "    def compute_op_durations(_logits):\n",
    "        transformed: np.ndarray = y_scaler.inverse_transform(_logits)\n",
    "        duration_dim = (0, 3)\n",
    "        durations = transformed[:, duration_dim[0]:duration_dim[1]].sum(axis=1)\n",
    "        return durations\n",
    "\n",
    "    graph_id_to_duration_pred = defaultdict(int)\n",
    "    for idx in range(batches_len):\n",
    "        inputs = input_batches[idx]\n",
    "        logits = output_batches[idx]\n",
    "        logits = nested_detach(logits)\n",
    "        logits = logits.cpu().numpy()\n",
    "        graph_ids = inputs[\"x_graph_id\"]\n",
    "        op_durations = compute_op_durations(logits)\n",
    "        for i, graph_id in enumerate(graph_ids):\n",
    "            op_duration = op_durations[i].item()\n",
    "            graph_id_to_duration_pred[graph_id] += op_duration\n",
    "    duration_metrics = MetricUtil.compute_duration_metrics(eval_graphs, graph_id_to_duration_pred)\n",
    "    return {\"eval_loss\": eval_loss, **duration_metrics}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def to_device(conf, features, labels):\n",
    "    features['x_op_feature'] = features[\"x_op_feature\"].to(device=conf.device)\n",
    "    labels['y_node_durations'] = labels['y_node_durations'].to(device=conf.device)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModel(MModule):\n",
    "\n",
    "    @staticmethod\n",
    "    def dimension_len(t):\n",
    "        return t[-1] - t[0]\n",
    "\n",
    "    def __init__(self, input_dimension, output_dimension, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.input = torch.nn.Linear(input_dimension, 128)\n",
    "        self.relu1 = ReLU()\n",
    "        self.dense1 = torch.nn.Linear(128, 64)\n",
    "        self.relu2 = ReLU()\n",
    "        self.dense2 = torch.nn.Linear(64, 32)\n",
    "        self.relu3 = ReLU()\n",
    "        self.output = torch.nn.Linear(32, output_dimension)\n",
    "        self.loss_fn = MSELoss()\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X[\"x_op_feature\"]\n",
    "        X = self.input(X)\n",
    "        X = self.relu1(X)\n",
    "        X = self.dense1(X)\n",
    "        X = self.relu2(X)\n",
    "        X = self.dense2(X)\n",
    "        X = self.relu3(X)\n",
    "        Y = self.output(X)\n",
    "        return Y\n",
    "\n",
    "    def compute_loss(self, outputs, Y):\n",
    "        node_durations = Y[\"y_node_durations\"]\n",
    "        loss = self.loss_fn(outputs, node_durations)\n",
    "        return loss\n",
    "\n",
    "\n",
    "def init_MLP_model(self) -> MModule | Any:\n",
    "    sample_preprocessed_ds = preprocessed_train_ds\n",
    "    sample_x_dict = sample_preprocessed_ds.features[0]\n",
    "    sample_y_dict = sample_preprocessed_ds.labels[0]\n",
    "    return MLPModel(input_dimension=len(sample_x_dict[\"x_op_feature\"]),\n",
    "                    output_dimension=len(sample_y_dict[\"y_node_durations\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PerfNetModel(MModule):\n",
    "    @staticmethod\n",
    "    def dimension_len(t):\n",
    "        return t[-1] - t[0]\n",
    "\n",
    "    def __init__(self, output_dimension, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.conv1 = torch.nn.LazyConv1d(out_channels=32, kernel_size=3, bias=True, padding_mode='zeros')\n",
    "        self.conv2 = torch.nn.LazyConv1d(out_channels=128, kernel_size=2, bias=True, padding_mode='zeros')\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.dense1 = torch.nn.LazyLinear(32)\n",
    "        self.relu1 = ReLU()\n",
    "        self.dense2 = torch.nn.LazyLinear(64)\n",
    "        self.relu2 = ReLU()\n",
    "        self.dense3 = torch.nn.LazyLinear(128)\n",
    "        self.relu3 = ReLU()\n",
    "        self.dense4 = torch.nn.LazyLinear(256)\n",
    "        self.relu4 = ReLU()\n",
    "        self.dropout = torch.nn.Dropout(p=0.3)\n",
    "        self.output = torch.nn.LazyLinear(output_dimension)\n",
    "        self.loss_fn = MSELoss()\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X[\"x_op_feature\"]\n",
    "        X = torch.unsqueeze(X, dim=1)\n",
    "        X = self.conv1(X)\n",
    "        X = self.conv2(X)\n",
    "        X = self.flatten(X)\n",
    "        X = self.dense1(X)\n",
    "        X = self.relu1(X)\n",
    "        X = self.dense2(X)\n",
    "        X = self.relu2(X)\n",
    "        X = self.dense3(X)\n",
    "        X = self.relu3(X)\n",
    "        X = self.dense4(X)\n",
    "        X = self.relu4(X)\n",
    "        X = self.dropout(X)\n",
    "        Y = self.output(X)\n",
    "        return Y\n",
    "\n",
    "    def compute_loss(self, outputs, Y):\n",
    "        node_durations = Y[\"y_node_durations\"]\n",
    "        loss = self.loss_fn(outputs, node_durations)\n",
    "        return loss\n",
    "\n",
    "\n",
    "def init_PerfNet_model(self) -> MModule | Any:\n",
    "    sample_y_dict = preprocessed_train_ds.labels[0]\n",
    "    return PerfNetModel(output_dimension=len(sample_y_dict[\"y_node_durations\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "init_model_funcs = {\n",
    "    ModelType.MLP: init_MLP_model,\n",
    "    ModelType.PerfNet: init_PerfNet_model,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = ModelType.MLP\n",
    "init_model = init_model_funcs[model_type]\n",
    "conf = train_configs[model_type]\n",
    "\n",
    "model = init_model(conf)\n",
    "model = model.to(conf.device)\n",
    "\n",
    "single_train_loop(conf, model, preprocessed_train_ds, preprocessed_eval_ds, compute_evaluate_metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLT-perf-model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

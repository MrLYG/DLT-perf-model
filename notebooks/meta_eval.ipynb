{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/DLT-perf-model/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import torch.nn.functional as F\n",
    "from abc import abstractmethod\n",
    "from collections import defaultdict\n",
    "from functools import lru_cache\n",
    "from itertools import count\n",
    "from typing import List, Dict\n",
    "from typing import Tuple, Any\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.nn import MSELoss, LSTM, GRU, RNN, L1Loss\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets_path: /root/guohao/DLT-perf-model/datasets\n",
      "configs_path: /root/guohao/DLT-perf-model/notebooks/configs\n",
      "datasets_path: /root/guohao/DLT-perf-model/datasets\n",
      "configs_path: /root/guohao/DLT-perf-model/notebooks/configs\n"
     ]
    }
   ],
   "source": [
    "from logger import init_logging\n",
    "from base_module import MModule\n",
    "from data import MDataset, Graph, load_graphs\n",
    "from importlib import reload\n",
    "from config import Config\n",
    "import config\n",
    "from data import MDataset, Graph, GraphNode, load_graphs, save_dataset_pkl, load_dataset_pkl, save_scalers_pkl, load_scalers_pkl\n",
    "import data\n",
    "from base_module import MModule, pad_np_vectors\n",
    "import base_module\n",
    "from executor import single_train_loop, nested_detach, grid_search_loop\n",
    "import executor\n",
    "from objects import ModelType\n",
    "import objects\n",
    "from metric import MetricUtil\n",
    "import metric\n",
    "from logger import init_logging, logging\n",
    "import logger\n",
    "import gcn\n",
    "from gcn import GCNLayer\n",
    "import transformer\n",
    "from transformer import TransformerModel\n",
    "reload(config)\n",
    "reload(data)\n",
    "reload(base_module)\n",
    "reload(executor)\n",
    "reload(objects)\n",
    "reload(metric)\n",
    "reload(logger)\n",
    "reload(gcn)\n",
    "reload(transformer)\n",
    "init_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_environment_str = \"RTX3080Ti_CPUALL\"\n",
    "scaler_cls = StandardScaler  # MinMaxScaler\n",
    "dummy = False\n",
    "model_type = ModelType.LSTM\n",
    "method_prefix = \"SubgraphBased\"\n",
    "eval_size = 200_000\n",
    "train_size = 10000\n",
    "epoch = 100\n",
    "eval_steps = 3000\n",
    "meta_model_path = 'ckpts/meta/LSTM/meta_train2024-01-04_03-39-58/ckpt_900.pth'\n",
    "scalers = None\n",
    "eval_graphs = None\n",
    "# 配置\n",
    "train_sizes = [10000, 50000, 100000, 150000, 200000, 250000, 300000]\n",
    "epoches = [300, 250, 200, 150, 100, 100, 100]\n",
    "eval_steps = [2000, 2000, 3000, 3000, 5000, 5000, 5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scalers(raw_train_ds: MDataset):\n",
    "\n",
    "    def _preprocess_required_data(ds: MDataset):\n",
    "        x_subgraph_feature_array = list()\n",
    "        y_nodes_durations_array = list()\n",
    "        y_subgraph_durations_array = list()\n",
    "\n",
    "        for data in ds:\n",
    "            feature, label = data\n",
    "            x_subgraph_feature = feature[\"x_subgraph_feature\"]\n",
    "            assert isinstance(x_subgraph_feature, list)\n",
    "            x_subgraph_feature_array.extend(x_subgraph_feature)\n",
    "\n",
    "            y_nodes_durations = label[\"y_nodes_durations\"]\n",
    "            assert isinstance(y_nodes_durations, list)\n",
    "            y_nodes_durations_array.extend(y_nodes_durations)\n",
    "\n",
    "            y_subgraph_durations = label[\"y_subgraph_durations\"]\n",
    "            y_subgraph_durations_array.append(y_subgraph_durations)\n",
    "\n",
    "        x_subgraph_feature_array = np.array(x_subgraph_feature_array)\n",
    "        y_nodes_durations_array = np.array(y_nodes_durations_array)\n",
    "        y_subgraph_durations_array = np.array(y_subgraph_durations_array)\n",
    "        return [x_subgraph_feature_array, y_nodes_durations_array, y_subgraph_durations_array]\n",
    "\n",
    "    x_subgraph_feature_array, y_nodes_durations_array, y_subgraph_durations_array = _preprocess_required_data(\n",
    "        ds=raw_train_ds)\n",
    "\n",
    "    x_subgraph_feature_scaler = scaler_cls()\n",
    "    x_subgraph_feature_scaler.fit(x_subgraph_feature_array)\n",
    "\n",
    "    y_nodes_durations_scaler = scaler_cls()\n",
    "    y_nodes_durations_scaler.fit(y_nodes_durations_array)\n",
    "\n",
    "    y_subgraph_durations_scaler = scaler_cls()\n",
    "    y_subgraph_durations_scaler.fit(y_subgraph_durations_array)\n",
    "\n",
    "    return x_subgraph_feature_scaler, y_nodes_durations_scaler, y_subgraph_durations_scaler\n",
    "\n",
    "\n",
    "def subgraph_features(graph: Graph, subgraph_node_size: int = 10, step: int = 5, dataset_params: Dict = {}) -> \\\n",
    "        Tuple[List[Dict], List[Dict]]:\n",
    "    subgraphs, _ = graph.subgraphs(\n",
    "        subgraph_node_size=subgraph_node_size, step=step)\n",
    "    X, Y = list(), list()\n",
    "\n",
    "    def subgraph_feature(nodes: List[GraphNode]):\n",
    "        feature_matrix = list()\n",
    "        for node in nodes:\n",
    "            feature = node.op.to_feature_array(\n",
    "                mode=dataset_params.get(\"mode\", \"complex\"))\n",
    "            feature = np.array(feature)\n",
    "            feature_matrix.append(feature)\n",
    "\n",
    "        feature_matrix = pad_np_vectors(feature_matrix)\n",
    "        feature_matrix = np.array(feature_matrix)\n",
    "\n",
    "        adj_matrix = [\n",
    "            [0.] * len(nodes) for _ in range(len(nodes))\n",
    "        ]\n",
    "        for curr_idx, node in enumerate(nodes):\n",
    "            if curr_idx + 1 < len(nodes):\n",
    "                adj_matrix[curr_idx][curr_idx+1] = 1.\n",
    "\n",
    "        adj_matrix = np.array(adj_matrix)\n",
    "        # x\n",
    "        feature = {\n",
    "            \"x_graph_id\": graph.ID,\n",
    "            \"x_node_ids\": \"|\".join([str(node.node_id) for node in nodes]),\n",
    "            \"x_subgraph_feature\": feature_matrix,\n",
    "            \"x_adj_matrix\": adj_matrix\n",
    "        }\n",
    "\n",
    "        # y\n",
    "        subgraph_duration = sum(node.duration + node.gap for node in subgraph)\n",
    "        nodes_durations = list()\n",
    "        for node in subgraph:\n",
    "            node_duration_label = (\n",
    "                node.duration, node.gap\n",
    "            )\n",
    "            nodes_durations.append(node_duration_label)\n",
    "\n",
    "        label = {\n",
    "            \"y_graph_id\": graph.ID,\n",
    "            \"y_nodes_durations\": nodes_durations,\n",
    "            \"y_subgraph_durations\": (subgraph_duration,)\n",
    "        }\n",
    "\n",
    "        return feature, label\n",
    "\n",
    "    for i, subgraph in enumerate(subgraphs):\n",
    "        x, y = subgraph_feature(subgraph)\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def init_dataset(graphs: List[Graph], conf) -> MDataset:\n",
    "    X = list()\n",
    "    Y = list()\n",
    "\n",
    "    subgraph_feature_maxsize = 0\n",
    "\n",
    "    for graph in graphs:\n",
    "        X_, Y_ = subgraph_features(graph=graph,\n",
    "                                   subgraph_node_size=conf.dataset_subgraph_node_size,\n",
    "                                   step=conf.dataset_subgraph_step,\n",
    "                                   dataset_params=conf.dataset_params)\n",
    "        for x in X_:\n",
    "            subgraph_feature_size = len(x[\"x_subgraph_feature\"][0])\n",
    "            subgraph_feature_maxsize = max(\n",
    "                subgraph_feature_maxsize, subgraph_feature_size)\n",
    "\n",
    "        X.extend(X_)\n",
    "        Y.extend(Y_)\n",
    "\n",
    "    for x in X:\n",
    "        x[\"x_subgraph_feature\"] = pad_np_vectors(\n",
    "            x[\"x_subgraph_feature\"], maxsize=subgraph_feature_maxsize)\n",
    "\n",
    "    dataset = MDataset(X, Y)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_dataset(ds: MDataset, scalers) -> MDataset:\n",
    "    x_subgraph_feature_scaler, y_nodes_durations_scaler, y_subgraph_durations_scaler = scalers\n",
    "\n",
    "    processed_features = list()\n",
    "    processed_labels = list()\n",
    "\n",
    "    for data in ds:\n",
    "        feature, label = data\n",
    "        x_subgraph_feature = feature[\"x_subgraph_feature\"]\n",
    "        assert isinstance(x_subgraph_feature, list)\n",
    "        x_subgraph_feature = np.array(x_subgraph_feature).astype(np.float32)\n",
    "        transformed_x_subgraph_feature = x_subgraph_feature_scaler.transform(\n",
    "            x_subgraph_feature)\n",
    "\n",
    "        x_adj_matrix = feature[\"x_adj_matrix\"]\n",
    "        x_adj_matrix = np.array(x_adj_matrix).astype(np.float32)\n",
    "\n",
    "        y_nodes_durations = label[\"y_nodes_durations\"]\n",
    "        assert isinstance(y_nodes_durations, list)\n",
    "        y_nodes_durations = np.array(y_nodes_durations).astype(np.float32)\n",
    "        transformed_y_nodes_durations = y_nodes_durations_scaler.transform(\n",
    "            y_nodes_durations)\n",
    "\n",
    "        y_subgraph_durations = label[\"y_subgraph_durations\"]\n",
    "        y_subgraph_durations_array = (y_subgraph_durations,)\n",
    "        y_subgraph_durations_array = y_subgraph_durations_scaler.transform(\n",
    "            y_subgraph_durations_array)\n",
    "        transformed_y_subgraph_durations = y_subgraph_durations_array[0]\n",
    "\n",
    "        processed_features.append({\n",
    "            \"x_graph_id\": feature[\"x_graph_id\"],\n",
    "            \"x_node_ids\": feature[\"x_node_ids\"],\n",
    "            \"x_subgraph_feature\": torch.Tensor(transformed_x_subgraph_feature),\n",
    "            \"x_adj_matrix\": torch.Tensor(x_adj_matrix)\n",
    "        })\n",
    "\n",
    "        processed_labels.append({\n",
    "            \"y_graph_id\": label[\"y_graph_id\"],\n",
    "            \"y_nodes_durations\": torch.Tensor(transformed_y_nodes_durations),\n",
    "            \"y_subgraph_durations\": torch.Tensor(transformed_y_subgraph_durations)\n",
    "        })\n",
    "\n",
    "    ds = MDataset(processed_features, processed_labels)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_evaluate_metrics(input_batches, output_batches, eval_loss) -> Dict[str, float]:\n",
    "    def compute_graph_nodes_durations(outputs_, node_ids_str_):\n",
    "        x_subgraph_feature_scaler, y_nodes_durations_scaler, y_subgraph_durations_scaler = scalers\n",
    "        node_to_durations = defaultdict(list)\n",
    "        for i, output_ in enumerate(outputs_):\n",
    "            node_ids = node_ids_str_[i]\n",
    "            node_ids_ = node_ids.split(\"|\")\n",
    "            assert len(output_) == len(node_ids_)\n",
    "            transformed: np.ndarray = y_nodes_durations_scaler.inverse_transform(\n",
    "                output_)\n",
    "            for i, node_id in enumerate(node_ids_):\n",
    "                node_to_durations[node_id].append(np.sum(transformed[i]))\n",
    "        node_to_duration = {k: np.average(v)\n",
    "                            for k, v in node_to_durations.items()}\n",
    "        return node_to_duration\n",
    "\n",
    "    graph_id_to_node_to_duration = defaultdict(lambda: defaultdict(list))\n",
    "    for inputs, outputs in zip(input_batches, output_batches):\n",
    "        outputs = nested_detach(outputs)\n",
    "        outputs = outputs.cpu().numpy()\n",
    "        graph_ids = inputs[\"x_graph_id\"]\n",
    "        graph_groups = defaultdict(list)\n",
    "        for i, graph_id in enumerate(graph_ids):\n",
    "            graph_groups[graph_id].append(i)\n",
    "\n",
    "        for graph_id, indices in graph_groups.items():\n",
    "            group_x_node_ids = [v for i, v in enumerate(\n",
    "                inputs[\"x_node_ids\"]) if i in indices]\n",
    "            group_outputs = [v for i, v in enumerate(outputs) if i in indices]\n",
    "            node_to_durations = compute_graph_nodes_durations(\n",
    "                group_outputs, group_x_node_ids)\n",
    "            for node, duration in node_to_durations.items():\n",
    "                graph_id_to_node_to_duration[graph_id][node].append(duration)\n",
    "    graph_id_to_duration_pred = dict()\n",
    "    # TODO check this!!!\n",
    "    for graph_id, node_to_duration in graph_id_to_node_to_duration.items():\n",
    "        duration_pred = 0\n",
    "        for _, duration_preds in node_to_duration.items():\n",
    "            duration_pred += np.average(duration_preds)\n",
    "        graph_id_to_duration_pred[graph_id] = duration_pred\n",
    "    duration_metrics = MetricUtil.compute_duration_metrics(\n",
    "        eval_graphs, graph_id_to_duration_pred)\n",
    "    return {\"eval_loss\": eval_loss, **duration_metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def to_device(conf: Config, features, labels):\n",
    "    features['x_subgraph_feature'] = features['x_subgraph_feature'].to(\n",
    "        conf.device)\n",
    "    features['x_adj_matrix'] = features['x_adj_matrix'].to(conf.device)\n",
    "    labels['y_nodes_durations'] = labels['y_nodes_durations'].to(conf.device)\n",
    "    labels['y_subgraph_durations'] = labels['y_subgraph_durations'].to(\n",
    "        conf.device)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTMModel(MModule):\n",
    "    def __init__(self, feature_size, nodes_durations_len, num_layers, bidirectional, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.lstm = LSTM(input_size=feature_size, hidden_size=feature_size, num_layers=num_layers, batch_first=True,\n",
    "                         bidirectional=bidirectional)\n",
    "        num_directions = 2 if bidirectional else 1\n",
    "        self.project = torch.nn.Linear(\n",
    "            in_features=feature_size * num_directions, out_features=nodes_durations_len)\n",
    "        self.loss_fn = L1Loss()\n",
    "\n",
    "    @staticmethod\n",
    "    def grid_search_model_params() -> Dict[str, List[Any]]:\n",
    "        return {\n",
    "            \"num_layers\": [4, 6, 8],\n",
    "            \"bidirectional\": [True, False],\n",
    "            \"learning_rate\": [1e-4, 1e-5],\n",
    "            'batch_size': [32, 64],\n",
    "            'epochs': [20],\n",
    "            'optimizer': ['Adam', 'SGD'],\n",
    "        }\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X[\"x_subgraph_feature\"]\n",
    "        out, _ = self.lstm(X)\n",
    "        Y = self.project(out)\n",
    "        return Y\n",
    "\n",
    "    def compute_loss(self, outputs, Y):\n",
    "        node_durations = Y[\"y_nodes_durations\"]\n",
    "        loss = self.loss_fn(outputs, node_durations)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-05 07:57:24,825] {data.py:448} INFO - Loading graphs eval\n",
      "[2024-01-05 07:57:24,826] {data.py:419} INFO - Loading merged.csv\n",
      "[2024-01-05 07:57:24,826] {data.py:419} INFO - Loading merged.csv\n",
      "[2024-01-05 07:57:25,314] {data.py:422} INFO - Loaded merged.csv, 200000 rows\n",
      "[2024-01-05 07:57:25,685] {data.py:428} INFO - Loaded resnet50.40_7.csv, 818 rows\n",
      "[2024-01-05 07:57:26,842] {data.py:428} INFO - Loaded efficientnet_b5.44_7.csv, 2707 rows\n",
      "[2024-01-05 07:57:26,994] {data.py:428} INFO - Loaded resnet18.56_7.csv, 317 rows\n",
      "[2024-01-05 07:57:27,447] {data.py:428} INFO - Loaded rand_3500.90_7.csv, 990 rows\n",
      "[2024-01-05 07:57:28,604] {data.py:428} INFO - Loaded efficientnet_b4.22_7.csv, 2234 rows\n",
      "[2024-01-05 07:57:29,265] {data.py:428} INFO - Loaded inception_v3.19_7.csv, 1466 rows\n",
      "[2024-01-05 07:57:29,610] {data.py:428} INFO - Loaded mobilenet_v3_small.57_7.csv, 734 rows\n",
      "[2024-01-05 07:57:29,972] {data.py:428} INFO - Loaded mnasnet0_75.74_7.csv, 769 rows\n",
      "[2024-01-05 07:57:31,123] {data.py:428} INFO - Loaded rand_2500.51_7.csv, 2539 rows\n",
      "[2024-01-05 07:57:31,756] {data.py:428} INFO - Loaded inception_v3.24_7.csv, 1466 rows\n",
      "[2024-01-05 07:57:32,137] {data.py:428} INFO - Loaded shufflenet_v2_x1_5.64_7.csv, 849 rows\n",
      "[2024-01-05 07:57:32,481] {data.py:428} INFO - Loaded mnasnet1_0.56_7.csv, 769 rows\n",
      "[2024-01-05 07:57:33,652] {data.py:428} INFO - Loaded efficientnet_b5.26_7.csv, 2709 rows\n",
      "[2024-01-05 07:57:34,243] {data.py:428} INFO - Loaded rand_5500.47_7.csv, 1343 rows\n",
      "[2024-01-05 07:57:34,608] {data.py:428} INFO - Loaded wide_resnet50_2.60_7.csv, 818 rows\n",
      "[2024-01-05 07:57:35,313] {data.py:428} INFO - Loaded efficientnet_b2.32_7.csv, 1604 rows\n",
      "[2024-01-05 07:57:35,379] {data.py:428} INFO - Loaded vgg11.36_7.csv, 125 rows\n",
      "[2024-01-05 07:57:35,631] {data.py:428} INFO - Loaded resnet34.46_7.csv, 565 rows\n",
      "[2024-01-05 07:57:35,879] {data.py:428} INFO - Loaded resnet34.32_7.csv, 563 rows\n",
      "[2024-01-05 07:57:35,953] {data.py:428} INFO - Loaded vgg13.36_7.csv, 145 rows\n",
      "[2024-01-05 07:57:36,455] {data.py:428} INFO - Loaded rand_1500.10_7.csv, 679 rows\n",
      "[2024-01-05 07:57:37,027] {data.py:428} INFO - Loaded rand_3500.76_7.csv, 1302 rows\n",
      "[2024-01-05 07:57:37,161] {data.py:428} INFO - Loaded squeezenet1_1.23_7.csv, 280 rows\n",
      "[2024-01-05 07:57:37,498] {data.py:428} INFO - Loaded mnasnet0_5.69_7.csv, 769 rows\n",
      "[2024-01-05 07:57:38,336] {data.py:428} INFO - Loaded rand_500.42_7.csv, 1962 rows\n",
      "[2024-01-05 07:57:39,154] {data.py:428} INFO - Loaded rand_1000.51_7.csv, 1913 rows\n",
      "[2024-01-05 07:57:40,231] {data.py:428} INFO - Loaded rand_500.87_7.csv, 2517 rows\n",
      "[2024-01-05 07:57:40,696] {data.py:428} INFO - Loaded rand_2000.38_7.csv, 1071 rows\n",
      "[2024-01-05 07:57:41,851] {data.py:428} INFO - Loaded efficientnet_b5.35_7.csv, 2709 rows\n",
      "[2024-01-05 07:57:42,189] {data.py:428} INFO - Loaded mobilenet_v2.35_7.csv, 771 rows\n",
      "[2024-01-05 07:57:42,510] {data.py:428} INFO - Loaded mobilenet_v3_small.58_7.csv, 734 rows\n",
      "[2024-01-05 07:57:43,199] {data.py:428} INFO - Loaded resnet101.31_7.csv, 1600 rows\n",
      "[2024-01-05 07:57:43,267] {data.py:428} INFO - Loaded vgg11.26_7.csv, 127 rows\n",
      "[2024-01-05 07:57:44,379] {data.py:428} INFO - Loaded rand_3500.31_7.csv, 2605 rows\n",
      "[2024-01-05 07:57:44,749] {data.py:428} INFO - Loaded shufflenet_v2_x1_5.29_7.csv, 849 rows\n",
      "[2024-01-05 07:57:45,085] {data.py:428} INFO - Loaded mnasnet0_5.60_7.csv, 769 rows\n",
      "[2024-01-05 07:57:45,240] {data.py:428} INFO - Loaded vgg19_bn.26_7.csv, 335 rows\n",
      "[2024-01-05 07:57:45,369] {data.py:428} INFO - Loaded squeezenet1_0.30_7.csv, 280 rows\n",
      "[2024-01-05 07:57:45,924] {data.py:428} INFO - Loaded resnet50.34_7.csv, 818 rows\n",
      "[2024-01-05 07:57:46,260] {data.py:428} INFO - Loaded mobilenet_v2.42_7.csv, 771 rows\n",
      "[2024-01-05 07:57:46,598] {data.py:428} INFO - Loaded mnasnet1_0.29_7.csv, 771 rows\n",
      "[2024-01-05 07:57:46,934] {data.py:428} INFO - Loaded mnasnet1_3.39_7.csv, 771 rows\n",
      "[2024-01-05 07:57:47,272] {data.py:428} INFO - Loaded mobilenet_v2.38_7.csv, 771 rows\n",
      "[2024-01-05 07:57:47,642] {data.py:428} INFO - Loaded shufflenet_v2_x1_0.42_7.csv, 849 rows\n",
      "[2024-01-05 07:57:47,770] {data.py:428} INFO - Loaded vgg16_bn.49_7.csv, 279 rows\n",
      "[2024-01-05 07:57:48,163] {data.py:428} INFO - Loaded googlenet.71_7.csv, 903 rows\n",
      "[2024-01-05 07:57:49,317] {data.py:428} INFO - Loaded efficientnet_b5.21_7.csv, 2709 rows\n",
      "[2024-01-05 07:57:49,638] {data.py:428} INFO - Loaded rand_1000.88_7.csv, 732 rows\n",
      "[2024-01-05 07:57:50,164] {data.py:428} INFO - Loaded rand_2500.49_7.csv, 1219 rows\n",
      "[2024-01-05 07:57:50,854] {data.py:428} INFO - Loaded efficientnet_b2.36_7.csv, 1602 rows\n",
      "[2024-01-05 07:57:51,541] {data.py:428} INFO - Loaded resnet101.33_7.csv, 1600 rows\n",
      "[2024-01-05 07:57:52,493] {data.py:428} INFO - Loaded efficientnet_b4.39_7.csv, 2234 rows\n",
      "[2024-01-05 07:57:52,849] {data.py:428} INFO - Loaded resnet50.26_7.csv, 818 rows\n",
      "[2024-01-05 07:57:52,979] {data.py:428} INFO - Loaded squeezenet1_0.22_7.csv, 280 rows\n",
      "[2024-01-05 07:57:53,300] {data.py:428} INFO - Loaded mobilenet_v3_small.53_7.csv, 736 rows\n",
      "[2024-01-05 07:57:53,385] {data.py:428} INFO - Loaded vgg16.44_7.csv, 175 rows\n",
      "[2024-01-05 07:57:53,778] {data.py:428} INFO - Loaded googlenet.34_7.csv, 905 rows\n",
      "[2024-01-05 07:57:54,147] {data.py:428} INFO - Loaded shufflenet_v2_x0_5.40_7.csv, 849 rows\n",
      "[2024-01-05 07:57:54,277] {data.py:428} INFO - Loaded squeezenet1_1.63_7.csv, 280 rows\n",
      "[2024-01-05 07:57:54,762] {data.py:428} INFO - Loaded efficientnet_b0.56_7.csv, 1127 rows\n",
      "[2024-01-05 07:57:55,132] {data.py:428} INFO - Loaded shufflenet_v2_x2_0.32_7.csv, 849 rows\n",
      "[2024-01-05 07:57:55,762] {data.py:428} INFO - Loaded inception_v3.26_7.csv, 1466 rows\n",
      "[2024-01-05 07:57:56,537] {data.py:428} INFO - Loaded efficientnet_b3.21_7.csv, 1814 rows\n",
      "[2024-01-05 07:57:56,873] {data.py:428} INFO - Loaded mobilenet_v2.47_7.csv, 771 rows\n",
      "[2024-01-05 07:57:58,039] {data.py:428} INFO - Loaded efficientnet_b4.41_7.csv, 2234 rows\n",
      "[2024-01-05 07:57:58,183] {data.py:428} INFO - Loaded resnet18.69_7.csv, 315 rows\n",
      "[2024-01-05 07:57:58,672] {data.py:428} INFO - Loaded efficientnet_b0.58_7.csv, 1129 rows\n",
      "[2024-01-05 07:57:59,783] {data.py:428} INFO - Loaded rand_6500.26_7.csv, 2605 rows\n",
      "[2024-01-05 07:58:00,770] {data.py:428} INFO - Loaded rand_5500.12_7.csv, 2313 rows\n",
      "[2024-01-05 07:58:01,105] {data.py:428} INFO - Loaded mnasnet0_5.66_7.csv, 769 rows\n",
      "[2024-01-05 07:58:01,780] {data.py:428} INFO - Loaded rand_6500.19_7.csv, 1567 rows\n",
      "[2024-01-05 07:58:02,101] {data.py:428} INFO - Loaded rand_0.62_7.csv, 735 rows\n",
      "[2024-01-05 07:58:02,466] {data.py:428} INFO - Loaded convnext_tiny.46_7.csv, 837 rows\n",
      "[2024-01-05 07:58:02,886] {data.py:428} INFO - Loaded rand_3000.16_7.csv, 972 rows\n",
      "[2024-01-05 07:58:03,663] {data.py:428} INFO - Loaded efficientnet_b3.32_7.csv, 1814 rows\n",
      "[2024-01-05 07:58:03,807] {data.py:428} INFO - Loaded resnet18.79_7.csv, 315 rows\n",
      "[2024-01-05 07:58:04,144] {data.py:428} INFO - Loaded mnasnet0_75.44_7.csv, 771 rows\n",
      "[2024-01-05 07:58:04,501] {data.py:428} INFO - Loaded resnet50.51_7.csv, 818 rows\n",
      "[2024-01-05 07:58:04,822] {data.py:428} INFO - Loaded mobilenet_v3_small.31_7.csv, 736 rows\n",
      "[2024-01-05 07:58:05,145] {data.py:428} INFO - Loaded mobilenet_v3_small.39_7.csv, 736 rows\n",
      "[2024-01-05 07:58:05,289] {data.py:428} INFO - Loaded resnet18.78_7.csv, 315 rows\n",
      "[2024-01-05 07:58:05,977] {data.py:428} INFO - Loaded resnet101.37_7.csv, 1600 rows\n",
      "[2024-01-05 07:58:06,665] {data.py:428} INFO - Loaded resnet101.56_7.csv, 1600 rows\n",
      "[2024-01-05 07:58:07,034] {data.py:428} INFO - Loaded shufflenet_v2_x2_0.42_7.csv, 849 rows\n",
      "[2024-01-05 07:58:07,723] {data.py:428} INFO - Loaded efficientnet_b1.30_7.csv, 1604 rows\n",
      "[2024-01-05 07:58:07,830] {data.py:428} INFO - Loaded vgg13_bn.27_7.csv, 227 rows\n",
      "[2024-01-05 07:58:08,501] {data.py:428} INFO - Loaded rand_5000.10_7.csv, 1563 rows\n",
      "[2024-01-05 07:58:09,278] {data.py:428} INFO - Loaded efficientnet_b3.33_7.csv, 1814 rows\n",
      "[2024-01-05 07:58:09,634] {data.py:428} INFO - Loaded rand_6500.85_7.csv, 817 rows\n",
      "[2024-01-05 07:58:10,002] {data.py:428} INFO - Loaded shufflenet_v2_x2_0.33_7.csv, 847 rows\n",
      "[2024-01-05 07:58:10,371] {data.py:428} INFO - Loaded shufflenet_v2_x2_0.68_7.csv, 849 rows\n",
      "[2024-01-05 07:58:10,739] {data.py:428} INFO - Loaded shufflenet_v2_x0_5.84_7.csv, 847 rows\n",
      "[2024-01-05 07:58:11,204] {data.py:428} INFO - Loaded rand_0.19_7.csv, 1073 rows\n",
      "[2024-01-05 07:58:11,333] {data.py:428} INFO - Loaded squeezenet1_1.68_7.csv, 280 rows\n",
      "[2024-01-05 07:58:11,439] {data.py:428} INFO - Loaded vgg13_bn.17_7.csv, 225 rows\n",
      "[2024-01-05 07:58:12,066] {data.py:428} INFO - Loaded shufflenet_v2_x1_0.52_7.csv, 849 rows\n",
      "[2024-01-05 07:58:12,424] {data.py:428} INFO - Loaded wide_resnet50_2.32_7.csv, 818 rows\n",
      "[2024-01-05 07:58:13,308] {data.py:428} INFO - Loaded rand_4000.45_7.csv, 1968 rows\n",
      "[2024-01-05 07:58:13,681] {data.py:428} INFO - Loaded mnasnet1_0.21_7.csv, 771 rows\n",
      "[2024-01-05 07:58:13,840] {data.py:428} INFO - Loaded resnet18.24_7.csv, 315 rows\n",
      "[2024-01-05 07:58:14,562] {data.py:428} INFO - Loaded wide_resnet101_2.55_7.csv, 1600 rows\n",
      "[2024-01-05 07:58:15,311] {data.py:428} INFO - Loaded convnext_base.22_7.csv, 1628 rows\n",
      "[2024-01-05 07:58:15,701] {data.py:428} INFO - Loaded convnext_tiny.52_7.csv, 837 rows\n",
      "[2024-01-05 07:58:16,800] {data.py:428} INFO - Loaded efficientnet_v2_s.22_7.csv, 2412 rows\n",
      "[2024-01-05 07:58:17,630] {data.py:428} INFO - Loaded efficientnet_b3.55_7.csv, 1812 rows\n",
      "[2024-01-05 07:58:17,698] {data.py:428} INFO - Loaded vgg11.55_7.csv, 125 rows\n",
      "[2024-01-05 07:58:17,965] {data.py:428} INFO - Loaded resnet34.21_7.csv, 565 rows\n",
      "[2024-01-05 07:58:18,364] {data.py:428} INFO - Loaded convnext_tiny.24_7.csv, 857 rows\n",
      "[2024-01-05 07:58:19,520] {data.py:428} INFO - Loaded densenet169.38_7.csv, 2633 rows\n",
      "[2024-01-05 07:58:20,643] {data.py:428} INFO - Loaded densenet169.20_7.csv, 2633 rows\n",
      "[2024-01-05 07:58:21,714] {data.py:428} INFO - Loaded densenet161.30_7.csv, 2507 rows\n",
      "[2024-01-05 07:58:22,235] {data.py:428} INFO - Loaded regnet_y_400mf.29_7.csv, 1200 rows\n",
      "[2024-01-05 07:58:22,326] {data.py:428} INFO - Loaded vgg11_bn.35_7.csv, 189 rows\n",
      "[2024-01-05 07:58:22,694] {data.py:428} INFO - Loaded shufflenet_v2_x2_0.71_7.csv, 847 rows\n",
      "[2024-01-05 07:58:23,555] {data.py:428} INFO - Loaded rand_2500.40_7.csv, 2007 rows\n",
      "[2024-01-05 07:58:23,700] {data.py:428} INFO - Loaded resnet18.47_7.csv, 317 rows\n",
      "[2024-01-05 07:58:24,068] {data.py:428} INFO - Loaded shufflenet_v2_x2_0.21_7.csv, 847 rows\n",
      "[2024-01-05 07:58:24,437] {data.py:428} INFO - Loaded shufflenet_v2_x1_0.66_7.csv, 849 rows\n",
      "[2024-01-05 07:58:24,687] {data.py:428} INFO - Loaded resnet34.44_7.csv, 565 rows\n",
      "[2024-01-05 07:58:25,011] {data.py:428} INFO - Loaded mobilenet_v3_small.30_7.csv, 736 rows\n",
      "[2024-01-05 07:58:25,162] {data.py:428} INFO - Loaded vgg19_bn.39_7.csv, 333 rows\n",
      "[2024-01-05 07:58:25,292] {data.py:428} INFO - Loaded vgg16_bn.27_7.csv, 281 rows\n",
      "[2024-01-05 07:58:25,660] {data.py:428} INFO - Loaded shufflenet_v2_x1_5.42_7.csv, 849 rows\n",
      "[2024-01-05 07:58:25,997] {data.py:428} INFO - Loaded mobilenet_v2.22_7.csv, 771 rows\n",
      "[2024-01-05 07:58:26,316] {data.py:428} INFO - Loaded rand_5000.65_7.csv, 729 rows\n",
      "[2024-01-05 07:58:26,672] {data.py:428} INFO - Loaded resnet50.58_7.csv, 816 rows\n",
      "[2024-01-05 07:58:27,027] {data.py:428} INFO - Loaded resnet50.20_7.csv, 818 rows\n",
      "[2024-01-05 07:58:28,208] {data.py:428} INFO - Loaded rand_2500.67_7.csv, 2776 rows\n",
      "[2024-01-05 07:58:28,696] {data.py:428} INFO - Loaded efficientnet_b0.45_7.csv, 1129 rows\n",
      "[2024-01-05 07:58:29,184] {data.py:428} INFO - Loaded efficientnet_b0.23_7.csv, 1129 rows\n",
      "[2024-01-05 07:58:29,269] {data.py:428} INFO - Loaded vgg16.34_7.csv, 173 rows\n",
      "[2024-01-05 07:58:29,637] {data.py:428} INFO - Loaded shufflenet_v2_x0_5.75_7.csv, 847 rows\n",
      "[2024-01-05 07:58:30,940] {data.py:428} INFO - Loaded rand_1000.32_7.csv, 2298 rows\n",
      "[2024-01-05 07:58:31,026] {data.py:428} INFO - Loaded vgg16.55_7.csv, 175 rows\n",
      "[2024-01-05 07:58:31,091] {data.py:428} INFO - Loaded vgg11.20_7.csv, 127 rows\n",
      "[2024-01-05 07:58:31,341] {data.py:428} INFO - Loaded resnet34.53_7.csv, 565 rows\n",
      "[2024-01-05 07:58:31,772] {data.py:428} INFO - Loaded rand_5000.53_7.csv, 990 rows\n",
      "[2024-01-05 07:58:32,129] {data.py:428} INFO - Loaded resnet50.62_7.csv, 816 rows\n",
      "[2024-01-05 07:58:32,466] {data.py:428} INFO - Loaded mnasnet1_0.34_7.csv, 771 rows\n",
      "[2024-01-05 07:58:32,854] {data.py:428} INFO - Loaded mobilenet_v3_large.47_7.csv, 888 rows\n",
      "[2024-01-05 07:58:33,217] {data.py:428} INFO - Loaded convnext_tiny.49_7.csv, 837 rows\n",
      "[2024-01-05 07:58:33,901] {data.py:428} INFO - Loaded convnext_large.26_7.csv, 1595 rows\n",
      "[2024-01-05 07:58:34,271] {data.py:428} INFO - Loaded shufflenet_v2_x0_5.33_7.csv, 849 rows\n",
      "[2024-01-05 07:58:34,607] {data.py:428} INFO - Loaded mnasnet1_3.20_7.csv, 771 rows\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(len(eval_steps)):\n",
    "    train_size = train_sizes[i]\n",
    "    epoch = epoches[i]\n",
    "    eval_step = epoches[i]\n",
    "    conf = Config.from_dict({\n",
    "        \"model\": \"LSTM\",\n",
    "        \"dataset_environment_str\": dataset_environment_str,\n",
    "        \"meta_dataset_environment_strs\": [dataset_environment_str],\n",
    "        \"dataset_subgraph_node_size\": 12,\n",
    "        \"dataset_subgraph_step\": 3,\n",
    "        \"all_seed\": 42,\n",
    "        \"dataset_params\": {\n",
    "            \"duration_summed\": False,\n",
    "        },\n",
    "        \"model_params\": {\n",
    "            \"num_layers\": 4,\n",
    "            \"bidirectional\": True\n",
    "        },\n",
    "        \"dataset_dummy\": False,\n",
    "        \"batch_size\": 128,\n",
    "        \"eval_steps\": eval_step,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"epochs\": epoch,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"train_size\": train_size,\n",
    "        \"eval_size\": eval_size,\n",
    "        \"meta_configs\": {\n",
    "            \"learning_rate\": 0.005,\n",
    "            \"meta_learning_rate\": 0.001,\n",
    "            \"meta_train_steps\": 1000,\n",
    "            \"meta_task_per_step\": 8,\n",
    "            \"meta_fast_adaption_step\": 5,\n",
    "            \"meta_dataset_train_environment_strs\": [dataset_environment_str],\n",
    "            \"meta_dataset_eval_environment_strs\": [dataset_environment_str],\n",
    "        },\n",
    "    })\n",
    "\n",
    "    eval_graphs = load_graphs(dataset_environment_str,\n",
    "                              train_or_eval=\"eval\",\n",
    "                              use_dummy=dummy,\n",
    "                              max_row=eval_size)\n",
    "    train_graphs = load_graphs(dataset_environment_str,\n",
    "                               train_or_eval=\"train\",\n",
    "                               use_dummy=dummy,\n",
    "                               max_row=train_size)  # todo 500\n",
    "    print(len(train_graphs), len(eval_graphs))\n",
    "    train_ds = init_dataset(train_graphs, conf)\n",
    "    eval_ds = init_dataset(eval_graphs, conf)\n",
    "    scalers = get_scalers(train_ds)\n",
    "    preprocessed_train_ds = preprocess_dataset(train_ds, scalers)\n",
    "    preprocessed_eval_ds = preprocess_dataset(eval_ds, scalers)\n",
    "    model = torch.load(meta_model_path)\n",
    "    model_type = ModelType.LSTM\n",
    "    single_train_loop(model_type, conf, preprocessed_train_ds,\n",
    "                      preprocessed_eval_ds, model, compute_evaluate_metrics, to_device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLT-pref-model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "from abc import abstractmethod\n",
    "from collections import defaultdict\n",
    "from functools import lru_cache\n",
    "from itertools import count\n",
    "from typing import List, Dict\n",
    "from typing import Tuple, Any\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.nn import MSELoss, LSTM, GRU\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "from config import Config\n",
    "import config\n",
    "from data import MDataset, Graph, GraphNode, load_graphs, save_dataset_pkl, load_dataset_pkl, save_scalers_pkl, load_scalers_pkl\n",
    "import data\n",
    "from base_module import MModule\n",
    "import base_module\n",
    "from executor import single_train_loop, nested_detach, pad_np_vectors\n",
    "import executor\n",
    "from objects import ModelType\n",
    "import objects\n",
    "from metric import MetricUtil\n",
    "import metric\n",
    "from logger import init_logging, logging\n",
    "import logger\n",
    "import gcn\n",
    "from gcn import GCNLayer\n",
    "import transformer\n",
    "from transformer import TransformerModel\n",
    "reload(config)\n",
    "reload(data)\n",
    "reload(base_module)\n",
    "reload(executor)\n",
    "reload(objects)\n",
    "reload(metric)\n",
    "reload(logger)\n",
    "reload(gcn)\n",
    "reload(transformer)\n",
    "from config import Config\n",
    "from data import MDataset, Graph, load_graphs\n",
    "from base_module import MModule\n",
    "from objects import ModelType\n",
    "from metric import MetricUtil\n",
    "from logger import init_logging\n",
    "from gcn import GCNLayer\n",
    "from transformer import TransformerModel\n",
    "init_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_environment_str = \"RTX2080Ti_CPU100\"\n",
    "normalizer_cls = StandardScaler # MinMaxScaler\n",
    "dummy = False\n",
    "model_type = ModelType.MLPTestSubgraph\n",
    "method_prefix = \"SubgraphBased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eval_graphs = load_graphs(dataset_environment_str,\n",
    "                            train_or_eval=\"train\",\n",
    "                            use_dummy=dummy,\n",
    "                            max_row=1_000)\n",
    "train_graphs = load_graphs(dataset_environment_str,\n",
    "                            train_or_eval=\"train\",\n",
    "                            use_dummy=dummy,\n",
    "                            max_row=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_configs = {\n",
    "    ModelType.MLPTestSubgraph.name: {\n",
    "        \"model\": \"MLPTestSubgraph\",\n",
    "        \"all_seed\": 42,\n",
    "        \"dataset_environment_str\": \"T4_CPU100\",\n",
    "        \"dataset_normalization\": \"MinMax\",\n",
    "        \"dataset_params\": {\n",
    "            \"duration_summed\": False,\n",
    "        },\n",
    "        \"dataset_dummy\": False,\n",
    "        \"batch_size\": 16,\n",
    "        \"eval_steps\": 100,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"epochs\": 100,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"meta_configs\": {\n",
    "            \"learning_rate\": 0.005,\n",
    "            \"meta_learning_rate\": 0.001,\n",
    "            \"meta_train_steps\": 1000,\n",
    "            \"meta_task_per_step\": 8,\n",
    "            \"meta_fast_adaption_step\": 5,\n",
    "            \"meta_dataset_train_environment_strs\": [\"T4_CPU100\"],\n",
    "            \"meta_dataset_eval_environment_strs\": [\"T4_CPU100\"],\n",
    "        },\n",
    "    },\n",
    "    ModelType.LSTM.name: {\n",
    "        \"model\": \"LSTM\",\n",
    "        \"dataset_environment_str\": \"RTX2080Ti_CPU100\",\n",
    "        \"meta_dataset_environment_strs\": [\"RTX2080Ti_CPU100\"],\n",
    "        \"dataset_normalization\": \"Standard\",\n",
    "        \"dataset_subgraph_node_size\": 10,\n",
    "        \"all_seed\": 42,\n",
    "        \"dataset_params\": {\n",
    "            \"duration_summed\": False,\n",
    "        },\n",
    "        \"model_params\": {\n",
    "            \"num_layers\": 5,\n",
    "            \"bidirectional\": True\n",
    "        },\n",
    "        \"dataset_dummy\": True,\n",
    "        \"batch_size\": 16,\n",
    "        \"eval_steps\": 100,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"epochs\": 100,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"meta_configs\": {\n",
    "            \"learning_rate\": 0.005,\n",
    "            \"meta_learning_rate\": 0.001,\n",
    "            \"meta_train_steps\": 1000,\n",
    "            \"meta_task_per_step\": 8,\n",
    "            \"meta_fast_adaption_step\": 5,\n",
    "            \"meta_dataset_train_environment_strs\": [\"RTX2080Ti_CPU100\"],\n",
    "            \"meta_dataset_eval_environment_strs\": [\"RTX2080Ti_CPU100\"],\n",
    "        },\n",
    "    },\n",
    "    ModelType.GRU.name: {\n",
    "        \"model\": \"GRU\",\n",
    "        \"dataset_environment_str\": \"RTX2080Ti_CPU100\",\n",
    "        \"meta_dataset_environment_strs\": [\"RTX2080Ti_CPU100\"],\n",
    "        \"dataset_normalization\": \"Standard\",\n",
    "        \"dataset_subgraph_node_size\": 10,\n",
    "        \"all_seed\": 42,\n",
    "        \"dataset_params\": {\n",
    "            \"duration_summed\": False,\n",
    "        },\n",
    "        \"model_params\": {\n",
    "            \"num_layers\": 5,\n",
    "            \"bidirectional\": True\n",
    "        },\n",
    "        \"dataset_dummy\": True,\n",
    "        \"batch_size\": 16,\n",
    "        \"eval_steps\": 100,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"epochs\": 100,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"meta_configs\": {\n",
    "            \"learning_rate\": 0.005,\n",
    "            \"meta_learning_rate\": 0.001,\n",
    "            \"meta_train_steps\": 1000,\n",
    "            \"meta_task_per_step\": 8,\n",
    "            \"meta_fast_adaption_step\": 5,\n",
    "            \"meta_dataset_train_environment_strs\": [\"RTX2080Ti_CPU100\"],\n",
    "            \"meta_dataset_eval_environment_strs\": [\"RTX2080Ti_CPU100\"],\n",
    "        },\n",
    "    },\n",
    "    ModelType.GCNSubgraph.name: {\n",
    "        \"model\": \"GCNGrouping\",\n",
    "        \"dataset_environment_str\": \"RTX2080Ti_CPU100\",\n",
    "        \"dataset_normalization\": \"Standard\",\n",
    "        \"dataset_subgraph_node_size\": 10,\n",
    "        \"all_seed\": 42,\n",
    "        \"dataset_params\": {\n",
    "            \"duration_summed\": False,\n",
    "        },\n",
    "        \"dataset_dummy\": True,\n",
    "        \"batch_size\": 16,\n",
    "        \"eval_steps\": 100,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"epochs\": 100,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"meta_configs\": {\n",
    "            \"learning_rate\": 0.005,\n",
    "            \"meta_learning_rate\": 0.001,\n",
    "            \"meta_train_steps\": 1000,\n",
    "            \"meta_task_per_step\": 8,\n",
    "            \"meta_fast_adaption_step\": 5,\n",
    "            \"meta_dataset_train_environment_strs\": [\"RTX2080Ti_CPU100\"],\n",
    "            \"meta_dataset_eval_environment_strs\": [\"RTX2080Ti_CPU100\"],\n",
    "        },\n",
    "    },\n",
    "    ModelType.Transformer.name: {\n",
    "        \"model\": \"Transformer\",\n",
    "        \"dataset_environment_str\": \"RTX2080Ti_CPU100\",\n",
    "        \"dataset_normalization\": \"Standard\",\n",
    "        \"dataset_subgraph_node_size\": 10,\n",
    "        \"all_seed\": 42,\n",
    "        \"dataset_params\": {\n",
    "            \"duration_summed\": False,\n",
    "        },\n",
    "        \"model_params\": {\n",
    "            \"nlayers\": 6,\n",
    "            \"d_hid\": 64,\n",
    "            \"dropout\": 0.0\n",
    "        },\n",
    "        \"dataset_dummy\": True,\n",
    "        \"batch_size\": 16,\n",
    "        \"eval_steps\": 100,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"epochs\": 100,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"meta_configs\": {\n",
    "            \"learning_rate\": 0.005,\n",
    "            \"meta_learning_rate\": 0.001,\n",
    "            \"meta_train_steps\": 1000,\n",
    "            \"meta_task_per_step\": 8,\n",
    "            \"meta_fast_adaption_step\": 5,\n",
    "            \"meta_dataset_train_environment_strs\": [\"RTX2080Ti_CPU100\"],\n",
    "            \"meta_dataset_eval_environment_strs\": [\"RTX2080Ti_CPU100\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "conf: Config = train_configs[model_type.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgraph_features(graph: Graph, subgraph_node_size: int = 10, step: int = 5, dataset_params: Dict = {}) -> \\\n",
    "        Tuple[List[Dict], List[Dict]]:\n",
    "    subgraphs, _ = graph.subgraphs(subgraph_node_size=subgraph_node_size, step=step)\n",
    "    X, Y = list(), list()\n",
    "\n",
    "    def subgraph_feature(nodes: List[GraphNode]):\n",
    "        feature_matrix = list()\n",
    "        for node in nodes:\n",
    "            feature = node.op.to_feature_array(\n",
    "                mode=dataset_params.get(\"mode\", \"complex\"))\n",
    "            feature = np.array(feature)\n",
    "            feature_matrix.append(feature)\n",
    "\n",
    "        feature_matrix = pad_np_vectors(feature_matrix)\n",
    "        feature_matrix = np.array(feature_matrix)\n",
    "\n",
    "        adj_matrix = [\n",
    "            [0.] * len(nodes) for _ in range(len(nodes))\n",
    "        ]\n",
    "        for curr_idx, node in enumerate(nodes):\n",
    "            if curr_idx + 1 < len(nodes):\n",
    "                adj_matrix[curr_idx][curr_idx+1] = 1.\n",
    "\n",
    "        adj_matrix = np.array(adj_matrix)\n",
    "        # x\n",
    "        feature = {\n",
    "            \"x_graph_id\": graph.ID,\n",
    "            \"x_node_ids\": \"|\".join([str(node.node_id) for node in nodes]),\n",
    "            \"x_subgraph_feature\": feature_matrix,\n",
    "            \"x_adj_matrix\": adj_matrix\n",
    "        }\n",
    "\n",
    "        # y\n",
    "        subgraph_duration = sum(node.duration + node.gap for node in subgraph)\n",
    "        nodes_durations = list()\n",
    "        for node in subgraph:\n",
    "            node_duration_label = (\n",
    "                node.duration, node.gap\n",
    "            )\n",
    "            nodes_durations.append(node_duration_label)\n",
    "\n",
    "        label = {\n",
    "            \"y_graph_id\": graph.ID,\n",
    "            \"y_nodes_durations\": nodes_durations,\n",
    "            \"y_subgraph_durations\": (subgraph_duration,)\n",
    "        }\n",
    "\n",
    "        return feature, label\n",
    "\n",
    "    for i, subgraph in enumerate(subgraphs):\n",
    "        x, y = subgraph_feature(subgraph)\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def init_dataset(graphs: List[Graph]) -> MDataset:\n",
    "    X = list()\n",
    "    Y = list()\n",
    "\n",
    "    subgraph_feature_maxsize = 0\n",
    "\n",
    "    for graph in graphs:\n",
    "        X_, Y_ = subgraph_features(graph=graph,\n",
    "                                        subgraph_node_size=conf.dataset_subgraph_node_size,\n",
    "                                        step=conf.dataset_subgraph_step,\n",
    "                                        dataset_params=conf.dataset_params)\n",
    "        for x in X_:\n",
    "            subgraph_feature_size = len(x[\"x_subgraph_feature\"][0])\n",
    "            subgraph_feature_maxsize = max(subgraph_feature_maxsize, subgraph_feature_size)\n",
    "\n",
    "        X.extend(X_)\n",
    "        Y.extend(Y_)\n",
    "\n",
    "    for x in X:\n",
    "        x[\"x_subgraph_feature\"] = pad_np_vectors(x[\"x_subgraph_feature\"], maxsize=subgraph_feature_maxsize)\n",
    "\n",
    "    dataset = MDataset(X, Y)\n",
    "    return dataset\n",
    "\n",
    "train_ds = init_dataset(train_graphs)\n",
    "eval_ds = init_dataset(eval_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scalers(raw_train_ds: MDataset):\n",
    "\n",
    "    def _preprocess_required_data(ds: MDataset):\n",
    "        x_subgraph_feature_array = list()\n",
    "        y_nodes_durations_array = list()\n",
    "        y_subgraph_durations_array = list()\n",
    "\n",
    "        for data in ds:\n",
    "            feature, label = data\n",
    "            x_subgraph_feature = feature[\"x_subgraph_feature\"]\n",
    "            assert isinstance(x_subgraph_feature, list)\n",
    "            x_subgraph_feature_array.extend(x_subgraph_feature)\n",
    "\n",
    "            y_nodes_durations = label[\"y_nodes_durations\"]\n",
    "            assert isinstance(y_nodes_durations, list)\n",
    "            y_nodes_durations_array.extend(y_nodes_durations)\n",
    "\n",
    "            y_subgraph_durations = label[\"y_subgraph_durations\"]\n",
    "            y_subgraph_durations_array.append(y_subgraph_durations)\n",
    "\n",
    "        x_subgraph_feature_array = np.array(x_subgraph_feature_array)\n",
    "        y_nodes_durations_array = np.array(y_nodes_durations_array)\n",
    "        y_subgraph_durations_array = np.array(y_subgraph_durations_array)\n",
    "        return [x_subgraph_feature_array, y_nodes_durations_array, y_subgraph_durations_array]\n",
    "    \n",
    "    scaler_cls = conf.dataset_normalizer_cls\n",
    "\n",
    "    x_subgraph_feature_array, y_nodes_durations_array, y_subgraph_durations_array = _preprocess_required_data(\n",
    "        ds=raw_train_ds)\n",
    "\n",
    "    x_subgraph_feature_scaler = scaler_cls()\n",
    "    x_subgraph_feature_scaler.fit(x_subgraph_feature_array)\n",
    "\n",
    "    y_nodes_durations_scaler = scaler_cls()\n",
    "    y_nodes_durations_scaler.fit(y_nodes_durations_array)\n",
    "\n",
    "    y_subgraph_durations_scaler = scaler_cls()\n",
    "    y_subgraph_durations_scaler.fit(y_subgraph_durations_array)\n",
    "\n",
    "    return x_subgraph_feature_scaler, y_nodes_durations_scaler, y_subgraph_durations_scaler\n",
    "\n",
    "scalers = get_scalers(train_ds)\n",
    "x_subgraph_feature_scaler, y_nodes_durations_scaler, y_subgraph_durations_scaler = scalers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_dataset(ds: MDataset) -> MDataset:\n",
    "    x_subgraph_feature_scaler, y_nodes_durations_scaler, y_subgraph_durations_scaler = scalers\n",
    "\n",
    "    processed_features = list()\n",
    "    processed_labels = list()\n",
    "\n",
    "    for data in ds:\n",
    "        feature, label = data\n",
    "        x_subgraph_feature = feature[\"x_subgraph_feature\"]\n",
    "        assert isinstance(x_subgraph_feature, list)\n",
    "        x_subgraph_feature = np.array(x_subgraph_feature).astype(np.float32)\n",
    "        transformed_x_subgraph_feature = x_subgraph_feature_scaler.transform(x_subgraph_feature)\n",
    "\n",
    "        x_adj_matrix = feature[\"x_adj_matrix\"]\n",
    "        x_adj_matrix = np.array(x_adj_matrix).astype(np.float32)\n",
    "\n",
    "        y_nodes_durations = label[\"y_nodes_durations\"]\n",
    "        assert isinstance(y_nodes_durations, list)\n",
    "        y_nodes_durations = np.array(y_nodes_durations).astype(np.float32)\n",
    "        transformed_y_nodes_durations = y_nodes_durations_scaler.transform(y_nodes_durations)\n",
    "\n",
    "        y_subgraph_durations = label[\"y_subgraph_durations\"]\n",
    "        y_subgraph_durations_array = (y_subgraph_durations,)\n",
    "        y_subgraph_durations_array = y_subgraph_durations_scaler.transform(y_subgraph_durations_array)\n",
    "        transformed_y_subgraph_durations = y_subgraph_durations_array[0]\n",
    "\n",
    "        processed_features.append({\n",
    "            \"x_graph_id\": feature[\"x_graph_id\"],\n",
    "            \"x_node_ids\": feature[\"x_node_ids\"],\n",
    "            \"x_subgraph_feature\": torch.Tensor(transformed_x_subgraph_feature),\n",
    "            \"x_adj_matrix\": torch.Tensor(x_adj_matrix)\n",
    "        })\n",
    "\n",
    "        processed_labels.append({\n",
    "            \"y_graph_id\": label[\"y_graph_id\"],\n",
    "            \"y_nodes_durations\": torch.Tensor(transformed_y_nodes_durations),\n",
    "            \"y_subgraph_durations\": torch.Tensor(transformed_y_subgraph_durations)\n",
    "        })\n",
    "\n",
    "    ds = MDataset(processed_features, processed_labels)\n",
    "    return ds\n",
    "\n",
    "\n",
    "preprocessed_train_ds = preprocess_dataset(train_ds)\n",
    "preprocessed_eval_ds = preprocess_dataset(eval_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_dataset_pkl(preprocessed_train_ds, conf.dataset_environment, method_prefix, 'train',\n",
    "                         conf.dataset_normalization)\n",
    "save_dataset_pkl(preprocessed_eval_ds, conf.dataset_environment, method_prefix, 'eval',\n",
    "                         conf.dataset_normalization)\n",
    "save_scalers_pkl(scalers, conf.dataset_environment, method_prefix, 'train',\n",
    "                         conf.dataset_normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_train_ds = load_dataset_pkl(conf.dataset_environment, method_prefix, 'train', \n",
    "                                         conf.dataset_normalization)\n",
    "preprocessed_eval_ds = load_dataset_pkl(conf.dataset_environment, method_prefix, 'eval',\n",
    "                                        conf.dataset_normalization)\n",
    "scalers = load_scalers_pkl(conf.dataset_environment, method_prefix, 'train',\n",
    "                           conf.dataset_normalization)\n",
    "op_feature_scaler, y_scaler = scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_evaluate_metrics(input_batches, output_batches, eval_loss) -> Dict[str, float]:\n",
    "    def compute_graph_nodes_durations(outputs_, node_ids_str_):\n",
    "            x_subgraph_feature_scaler, y_nodes_durations_scaler, y_subgraph_durations_scaler = scalers\n",
    "            node_to_durations = defaultdict(list)\n",
    "            for i, output_ in enumerate(outputs_):\n",
    "                node_ids = node_ids_str_[i]\n",
    "                node_ids_ = node_ids.split(\"|\")\n",
    "                assert len(output_) == len(node_ids_)\n",
    "                transformed: np.ndarray = y_nodes_durations_scaler.inverse_transform(output_)\n",
    "                for i, node_id in enumerate(node_ids_):\n",
    "                    node_to_durations[node_id].append(np.sum(transformed[i]))\n",
    "            node_to_duration = {k: np.average(v) for k, v in node_to_durations.items()}\n",
    "            return node_to_duration\n",
    "\n",
    "    graph_id_to_node_to_duration = defaultdict(lambda: defaultdict(list))\n",
    "    for inputs, outputs in zip(input_batches, output_batches):\n",
    "        outputs = nested_detach(outputs)\n",
    "        outputs = outputs.cpu().numpy()\n",
    "        graph_ids = inputs[\"x_graph_id\"]\n",
    "        graph_groups = defaultdict(list)\n",
    "        for i, graph_id in enumerate(graph_ids):\n",
    "            graph_groups[graph_id].append(i)\n",
    "\n",
    "        for graph_id, indices in graph_groups.items():\n",
    "            group_x_node_ids = [v for i, v in enumerate(inputs[\"x_node_ids\"]) if i in indices]\n",
    "            group_outputs = [v for i, v in enumerate(outputs) if i in indices]\n",
    "            node_to_durations = compute_graph_nodes_durations(group_outputs, group_x_node_ids)\n",
    "            for node, duration in node_to_durations.items():\n",
    "                graph_id_to_node_to_duration[graph_id][node].append(duration)\n",
    "    graph_id_to_duration_pred = dict()\n",
    "    # TODO check this!!!\n",
    "    for graph_id, node_to_duration in graph_id_to_node_to_duration.items():\n",
    "        duration_pred = 0\n",
    "        for _, duration_preds in node_to_duration.items():\n",
    "            duration_pred += np.average(duration_preds)\n",
    "        graph_id_to_duration_pred[graph_id] = duration_pred\n",
    "    duration_metrics = MetricUtil.compute_duration_metrics(eval_graphs, graph_id_to_duration_pred)\n",
    "    return {\"eval_loss\": eval_loss, **duration_metrics}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def to_device(conf: Config, features, labels):\n",
    "    features['x_subgraph_feature'] = features['x_subgraph_feature'].to(conf.device)\n",
    "    features['x_adj_matrix'] = features['x_adj_matrix'].to(conf.device)\n",
    "    features['y_nodes_durations'] = features['y_nodes_durations'].to(conf.device)\n",
    "    features['y_subgraph_durations'] = features['y_subgraph_durations'].to(conf.device)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLPTest_SubgraphModel(MModule):\n",
    "\n",
    "    def __init__(self, x_node_feature_count, x_node_feature_size, y_nodes_duration_count, y_nodes_duration_size,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.x_node_feature_count, self.x_node_feature_size, self.y_nodes_duration_count, self.y_nodes_duration_size \\\n",
    "            = x_node_feature_count, x_node_feature_size, y_nodes_duration_count, y_nodes_duration_size\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.linear1 = torch.nn.Linear(in_features=self.x_node_feature_count * self.x_node_feature_size,\n",
    "                                       out_features=64)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(in_features=64,\n",
    "                                       out_features=32)\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "        self.output = torch.nn.Linear(32, self.y_nodes_duration_count * self.y_nodes_duration_size)\n",
    "        self.loss_fn = MSELoss()\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X[\"x_subgraph_feature\"]\n",
    "        X = self.flatten(X)\n",
    "        X = self.linear1(X)\n",
    "        X = self.relu1(X)\n",
    "        X = self.linear2(X)\n",
    "        X = self.relu2(X)\n",
    "        Y = self.output(X)\n",
    "        Y = torch.reshape(Y, (-1, self.y_nodes_duration_count, self.y_nodes_duration_size))\n",
    "        return Y\n",
    "\n",
    "    def compute_loss(self, outputs, Y):\n",
    "        nodes_durations = Y[\"y_nodes_durations\"]\n",
    "        loss = self.loss_fn(outputs, nodes_durations)\n",
    "        return loss\n",
    "\n",
    "def init_MLPTestSubgraph_model() -> MModule | Any:\n",
    "    sample_preprocessed_ds = preprocessed_train_ds\n",
    "    sample_x_dict = sample_preprocessed_ds.features[0]\n",
    "    sample_y_dict = sample_preprocessed_ds.labels[0]\n",
    "    x_node_feature_count = len(sample_x_dict[\"x_subgraph_feature\"])\n",
    "    x_node_feature_size = len(sample_x_dict[\"x_subgraph_feature\"][0])\n",
    "    y_nodes_duration_count = len(sample_y_dict[\"y_nodes_durations\"])\n",
    "    y_nodes_duration_size = len(sample_y_dict[\"y_nodes_durations\"][0])\n",
    "    return MLPTest_SubgraphModel(x_node_feature_count,\n",
    "                                    x_node_feature_size,\n",
    "                                    y_nodes_duration_count,\n",
    "                                    y_nodes_duration_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTMModel(MModule):\n",
    "    def __init__(self, feature_size, nodes_durations_len, num_layers, bidirectional, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.lstm = LSTM(input_size=feature_size, hidden_size=feature_size, num_layers=num_layers, batch_first=True,\n",
    "                         bidirectional=bidirectional)\n",
    "        num_directions = 2 if bidirectional else 1\n",
    "        self.project = torch.nn.Linear(in_features=feature_size * num_directions, out_features=nodes_durations_len)\n",
    "        self.loss_fn = MSELoss()\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X[\"x_subgraph_feature\"]\n",
    "        out, _ = self.lstm(X)\n",
    "        Y = self.project(out)\n",
    "        return Y\n",
    "\n",
    "    def compute_loss(self, outputs, Y):\n",
    "        node_durations = Y[\"y_nodes_durations\"]\n",
    "        loss = self.loss_fn(outputs, node_durations)\n",
    "        return loss\n",
    "\n",
    "def init_LSTM_model() -> MModule | Any:\n",
    "    def default_model_params() -> Dict[str, Any]:\n",
    "        nhead: int = 8\n",
    "        d_hid: int = 512\n",
    "        nlayers: int = 6\n",
    "        dropout: float = 0.5\n",
    "        return {\n",
    "            \"nhead\": nhead,\n",
    "            \"d_hid\": d_hid,\n",
    "            \"nlayers\": nlayers,\n",
    "            \"dropout\": dropout\n",
    "        }\n",
    "\n",
    "    sample_preprocessed_ds = preprocessed_train_ds\n",
    "    sample_x_dict = sample_preprocessed_ds.features[0]\n",
    "    sample_y_dict = sample_preprocessed_ds.labels[0]\n",
    "    x_node_feature_size = len(sample_x_dict[\"x_subgraph_feature\"][0])\n",
    "    y_nodes_durations_len = len(sample_y_dict[\"y_nodes_durations\"][0])\n",
    "    model_params = conf.model_params\n",
    "    final_params = default_model_params()\n",
    "    for k, v in final_params.items():\n",
    "        final_params[k] = model_params.get(k, v)\n",
    "    return LSTMModel(\n",
    "        feature_size=x_node_feature_size,\n",
    "        nodes_durations_len=y_nodes_durations_len,\n",
    "        **final_params\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GRUModel(MModule):\n",
    "    def __init__(self, feature_size, nodes_durations_len, num_layers, bidirectional, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.gru = GRU(input_size=feature_size, hidden_size=feature_size, num_layers=num_layers, batch_first=True,\n",
    "                       bidirectional=bidirectional)\n",
    "        num_directions = 2 if bidirectional else 1\n",
    "        self.project = torch.nn.Linear(in_features=feature_size * num_directions, out_features=nodes_durations_len)\n",
    "        self.loss_fn = MSELoss()\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X[\"x_subgraph_feature\"]\n",
    "        out, _ = self.gru(X)\n",
    "        Y = self.project(out)\n",
    "        return Y\n",
    "\n",
    "    def compute_loss(self, outputs, Y):\n",
    "        node_durations = Y[\"y_nodes_durations\"]\n",
    "        loss = self.loss_fn(outputs, node_durations)\n",
    "        return loss\n",
    "\n",
    "\n",
    "def init_GRU_model() -> MModule | Any:\n",
    "    def default_model_params() -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"num_layers\": 4,\n",
    "            \"bidirectional\": True,\n",
    "        }\n",
    "\n",
    "    sample_preprocessed_ds = preprocessed_train_ds\n",
    "    sample_x_dict = sample_preprocessed_ds.features[0]\n",
    "    sample_y_dict = sample_preprocessed_ds.labels[0]\n",
    "    x_node_feature_size = len(sample_x_dict[\"x_subgraph_feature\"][0])\n",
    "    y_nodes_durations_len = len(sample_y_dict[\"y_nodes_durations\"][0])\n",
    "    model_params = conf.model_params\n",
    "    final_params = default_model_params()\n",
    "    for k, v in final_params.items():\n",
    "        final_params[k] = model_params.get(k, v)\n",
    "    return GRUModel(\n",
    "        feature_size=x_node_feature_size,\n",
    "        nodes_durations_len=y_nodes_durations_len,\n",
    "        **final_params\n",
    "    )\n",
    "\n",
    "\n",
    "class GCNSubgraphModel(MModule):\n",
    "    def __init__(self, dim_feats, dim_h, dim_out, n_layers, dropout):\n",
    "        super(GCNSubgraphModel, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        # input layer\n",
    "        self.layers.append(GCNLayer(dim_feats, dim_h, F.relu, 0))\n",
    "        # hidden layers\n",
    "        for i in range(n_layers - 1):\n",
    "            self.layers.append(GCNLayer(dim_h, dim_h, F.relu, dropout))\n",
    "        # output layer\n",
    "        self.layers.append(GCNLayer(dim_h, dim_out, None, dropout))\n",
    "        self.loss_fn = MSELoss()\n",
    "\n",
    "    def forward(self, X):\n",
    "        adj, features = X[\"x_adj_matrix\"], X[\"x_subgraph_feature\"]\n",
    "        h = features\n",
    "        for layer in self.layers:\n",
    "            h = layer(adj, h)\n",
    "        return h\n",
    "\n",
    "    def compute_loss(self, outputs, Y) -> torch.Tensor:\n",
    "        y_nodes_durations = Y[\"y_nodes_durations\"]\n",
    "        loss = self.loss_fn(outputs, y_nodes_durations)\n",
    "        return loss\n",
    "\n",
    "\n",
    "def init_GCNSubgraph_model() -> MModule | Any:\n",
    "    def default_model_params() -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"dim_h\": None,\n",
    "            \"n_layers\": 2,\n",
    "            \"dropout\": 0.1,\n",
    "        }\n",
    "    sample_preprocessed_ds = preprocessed_train_ds\n",
    "    sample_x_dict = sample_preprocessed_ds.features[0]\n",
    "    sample_y_dict = sample_preprocessed_ds.labels[0]\n",
    "    x_node_feature_size = len(sample_x_dict[\"x_subgraph_feature\"][0])\n",
    "    y_nodes_durations_len = len(sample_y_dict[\"y_nodes_durations\"][0])\n",
    "    model_params = conf.model_params\n",
    "    final_params = default_model_params()\n",
    "    for k, v in final_params.items():\n",
    "        final_params[k] = model_params.get(k, v)\n",
    "    if final_params[\"dim_h\"] is None:\n",
    "        final_params[\"dim_h\"] = x_node_feature_size\n",
    "    return GCNSubgraphModel(\n",
    "        dim_feats=x_node_feature_size,\n",
    "        dim_out=y_nodes_durations_len,\n",
    "        **final_params\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_Transformer_model() -> MModule | Any:\n",
    "    def default_model_params() -> Dict[str, Any]:\n",
    "        nhead: int = 8\n",
    "        d_hid: int = 512\n",
    "        nlayers: int = 6\n",
    "        dropout: float = 0.5\n",
    "        return {\n",
    "            \"nhead\": nhead,\n",
    "            \"d_hid\": d_hid,\n",
    "            \"nlayers\": nlayers,\n",
    "            \"dropout\": dropout\n",
    "        }\n",
    "    sample_preprocessed_ds = preprocessed_train_ds\n",
    "    sample_x_dict = sample_preprocessed_ds.features[0]\n",
    "    sample_y_dict = sample_preprocessed_ds.labels[0]\n",
    "    x_node_feature_size = len(sample_x_dict[\"x_subgraph_feature\"][0])\n",
    "    nodes_durations_len = len(sample_y_dict[\"y_nodes_durations\"][0])\n",
    "    model_params = conf.model_params\n",
    "    final_params = default_model_params()\n",
    "    for k, v in final_params.items():\n",
    "        final_params[k] = model_params.get(k, v)\n",
    "\n",
    "    nhead = final_params[\"nhead\"]\n",
    "    while x_node_feature_size % nhead != 0:\n",
    "        nhead -= 1\n",
    "    if nhead != final_params[\"nhead\"]:\n",
    "        final_params[\"nhead\"] = nhead\n",
    "        logging.info(f\"Transformer nhead set to {nhead}.\")\n",
    "        conf.model_params[\"nhead\"] = nhead\n",
    "\n",
    "    return TransformerModel(\n",
    "        d_model=x_node_feature_size,\n",
    "        output_d=nodes_durations_len,\n",
    "        **final_params\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "init_model_funcs = {\n",
    "    ModelType.Transformer.name: init_Transformer_model,\n",
    "    ModelType.GCNSubgraph.name: init_GCNSubgraph_model,\n",
    "    ModelType.GRU.name: init_GRU_model,\n",
    "    ModelType.LSTM.name: init_LSTM_model,\n",
    "    ModelType.MLPTestSubgraph.name: init_MLPTestSubgraph_model,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = ModelType.MLPTestSubgraph\n",
    "init_model = init_model_funcs[model_type.name]\n",
    "\n",
    "model = init_model(conf)\n",
    "model = model.to(conf.device)\n",
    "\n",
    "single_train_loop(model_type, conf, preprocessed_train_ds, preprocessed_eval_ds, model, compute_evaluate_metrics, to_device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLT-perf-model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
